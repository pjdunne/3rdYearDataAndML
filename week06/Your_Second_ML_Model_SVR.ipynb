{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Your 2nd ML Model: The Support Vector Machine\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Outline <a id='outline'></a>\n",
    "1. [Section One: The Support Vector Machine](#section-one-the-support-vector-machine)\n",
    "1. [Section Two: A toy model with SVR](#section-two-a-toy-model-with-svr)\n",
    "1. [Section Three: SVM for non-linear problems](#section-three-non-linear-support-vector-machines)\n",
    "1. [Section Four: The California Housing dataset](#section-four-the-california-housing-dataset)\n",
    "1. [Section Five: Exercises](#section-five-exercises)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "## Section One: The Support Vector Machine [^](#outline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine (SVM) is one of the most commonly used ML techniques. SVMs can be used in classification or regression, although generally more the former. You briefly encountered an SVM for classification last week, although only seeing the sklearn syntax. This week you will see SVMs in more detail, including discussion of how they work, some of the mathematics, and how they can be applied to regression. For the exercises this week, you will be using support vector regression to estimate the median house value in a number of California districts, the dataset for which is discussed in [Section Four](#section-4).\n",
    "\n",
    "The following sections will provide an outline of the theoretical basis of SVM classification and regression, for linear problems. You will see non-linear problems tackled in [Section Three](#section-3)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM classification (SVC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear SVM classification, we typically deal with **linearly-separable** datasets. This refers to datasets that can be separated with a single line or plane or hyperplane, depending on the number of dimensions of the input feature space. \n",
    "\n",
    "For a general linearly-separable binary classification problem, we may be able to intuit a hyperplane that separates our two classes, as seen in the 2D example below. \n",
    "\n",
    "Here we have taken two of the classes from the IRIS dataset you saw last week, *iris versicolor* and *iris setosa*, and plotted the distribution of petal length against petal width. These two classes are linearly separable in these variables. The dotted and dashed lines illustrate possible separation lines.\n",
    "\n",
    "These possible separators are not necessarily robust against outliers; it is easy to imagine additional irises in either class crossing these classification lines, due to the spread of possible values. We need to optimise our choice of separation line to ensure we are robust against outliers and variation in our data.\n",
    "\n",
    "<img src=Week6_plots/arbitrary_separators_example.png align='center' height='600'>\n",
    "\n",
    "*Example possible separating lines for two of the classes in the IRIS dataset. While these lines do separate the two classes, they are not robust against outliers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The support vector machine attempts to address this problem. In general, we want to define some hyperplane that optimally separates the two classes. This hyperplane is referred to as the **maximum-margin hyperplane**. To define the maximum-margin hyperplane for linearly separable data, we first define two parallel hyerplanes that separate the two classes. The region these hyperplanes bound is referred to as the **margin**, and the maximum-margin hyperplane is the hyperlane exactly in the middle of the margin. For normalised data, the margin-bounding hyperplanes can be described by the following equations:\n",
    "\n",
    "* $\\mathbf{w}^\\intercal\\mathbf{X} - b = +1$: any points on or above this boundary are in the class with label +1\n",
    "\n",
    "* $\\mathbf{w}^\\intercal\\mathbf{X} - b = -1$: any points on or below this boundary are in the class with label -1\n",
    "\n",
    "Here, $\\mathbf{X}$ denotes any vector in the $d$-dimensional input space $\\mathbb{R}^d$. $\\mathbf{w}$ and $b$ are vector and scalar parameters of the hyperplanes respectively, and $\\mathbf{w}^\\intercal$ denotes the transpose of the vector $\\mathbf{w}$.\n",
    "\n",
    "The distance between these two planes is equal to $\\frac{2}{||\\mathbf{w}||}$, so to maximise the size of the margin we must minimise $||\\mathbf{w}||$. The function of SMV classifier training is therefore to optimise the class discrimination on the training set {$\\mathbf{X}_i$, $y_i$}  by minimising $||\\mathbf{w}||$. The specific problem to solve depends on the strictness of the margin, which is discussed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard-margin classification\n",
    "\n",
    "For linearly-separable data, we can impose the so-called **hard-margin** condition, i.e. that **no** data points are allowed inside the margin. The optimisation of the choice of hyperplane is subject to the requirement that we have no **margin violations**, i.e. no points within the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "\n",
    "Before, we simply outlined rules as to how we label points relative based on their position relative to the planes bounding the margin. The hard-margin condition can be expressed as \n",
    "\n",
    "* $\\mathbf{w}^\\intercal\\mathbf{X}_i - b \\geq 1, \\text{ for } y_i = 1$\n",
    "* $\\mathbf{w}^\\intercal\\mathbf{X}_i - b \\leq -1, \\text{ for } y_i = -1$\n",
    "\n",
    "This requires that all training points are outside of the margin. The two conditions can be combined into a single condition, as\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i \\cdot (\\mathbf{w}^\\intercal\\mathbf{X}_i - b) \\geq 1, \\quad \\forall\\,\\,i\n",
    "\\end{equation*}\n",
    "\n",
    "The optimisation problem that is solved during SVM classifier training is therefore completely defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w},\\, b}\\quad & \\frac{1}{2}||\\mathbf{w}||^2 \\\\\n",
    "\\text{subject to}\\quad & y_i\\cdot(\\mathbf{w}^\\intercal\\mathbf{X}_i - b) \\geq 1  ,\n",
    "\\end{align*}\n",
    "where $\\min\\limits_{\\mathbf{w{,\\, b}}}$ means that the minimisation is with respect to $\\mathbf{w}$ and $b$. While we previously said that we needed to minimise $||\\mathbf{w}||$, minimising $||\\mathbf{w}||^2$ is more numerically stable (the explanation of this is beyond the scope of this course) and as such is chosen for optimisation. The factor of 1/2 is added to make partial derivatives during the optimisation less messy. The values of $\\mathbf{w}$ and $b$ that solve this optimisation define the classifier.  \n",
    "\n",
    "The prediction for a test point $\\mathbf{X}_t$ is then given as $\\hat{y}_t$ = $\\text{sign}(\\mathbf{w}^\\intercal\\mathbf{X}_t - b)$. If the value of $\\mathbf{w}^\\intercal\\mathbf{X}_t - b$ is positive, then $\\hat{y}_t$ is assigned to be +1, and if $\\mathbf{w}^\\intercal\\mathbf{X}_t - b$ is negative then $\\hat{y}_t$ is assigned to be -1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The result of applying this analysis to the same IRIS data shown above can be seen in the figure below. The dashed lines indicate the boundaries of the margin and the solid line indicates the maximum-margin line (as this is a 2D problem we can refer to it as a line rather than a hyperplane). The maximum-margin line is only dependent on the points in each class closest to the line, which are referred to as the **support vectors** and are the namesake of the support vector machine.\n",
    "\n",
    "<img src=Week6_plots/svc_hard_iris.png align='center' height=600>\n",
    "\n",
    "*Hard margin classification using the IRIS dataset. The dashed lines indicate the margin boundaries and the points indicated in red are the **support vectors**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft-margin classification\n",
    "\n",
    "Of course, it may not always be possible to construct a maximum-margin hyperplane such that all of the data points are outside of the margin. This means that the data is not linearly separable, and results in so-called **margin violations** where training data points are inside the margin. You may also find that even if your data is linearly separable, it may be with a very small margin, which is not very resistant to outliers. To tackle such cases, the soft-margin condition is introduced. \n",
    "\n",
    "Rather than strictly requiring all points are on the boundary or outside of the margin, we can allow points inside the margin. We add a penalty term to the optimsation that penalises points for lying too far into the margin. The strength of this penalty is controlled by a parameter $C$, which is known as a **regularisation parameter**. This is a hyperparameter of the algorithm, and you will have seen it in sklearn last week. Low values of $C$ allow greater margin violations, while high values minimise margin values. Very high values of $C$ tend towards the hard-margin classification problem, i.e. no margin violations allowed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "For each point a small margin of error $\\zeta_i$ can be defined. This allows points to lie in the margin, with a penalty assigned for how far into the margin a given point lies. The corresponding optimisation problem can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w},\\, b,\\, \\zeta} \\quad & \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i = 1}^n\\zeta_i \\\\\n",
    "\\text{subject to} \\quad& \\begin{cases} & y_i\\cdot(\\mathbf{w}^\\intercal\\mathbf{X}_i - b)\\geq 1 - \\zeta_i, \\\\\n",
    " & \\zeta_i \\geq 0 \\quad \\forall\\,\\, i,\\end{cases}\n",
    "\\end{align*}\n",
    "where $C$ is a **hyperparameter** of the algorithm and determines how heavily margin violations are penalised. $C$ is known as the **regularisation parameter**. Low values of $C$ allow greater margin violations, while high values minimise margin violations. For very high values of $C$, this is very similar to the hard-margin classification problem. $C$ is a hyperparameter of algorithm and you will have seen it in sklearn last week.\n",
    "\n",
    "\n",
    "\n",
    "This additional constraint is equivalent to using a loss function known as the **hinge loss function**. This function is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "f(\\hat{y}_i, y_i) = \\max (0, 1 - \\hat{y}_i \\cdot y_i),\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\hat{y}_i$ denotes the prediction for sample $i$ and is equal to $\\mathbf{w}^\\intercal\\cdot\\mathbf{X}_i - b$, $y_i$ denotes the target for sample $i$, and $f(\\hat{y}_i, y_i)$ denotes the value of the hinge loss. This penalises predictions where $\\hat{y}_i$ and $y_i$ have different signs (as $\\hat{y}_i, y_i \\in \\{-1, +1\\}$)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have now formulated the so-called **primal** optimisation problem. In general, this is a hard-to-optimise problem; to make things easier, we can use a method of Lagrange multipliers to formulate the **dual** problem. For $n$ input samples with $p$ features each such that $\\mathbf{X}_i \\in \\mathbb{R}^p$ and $i = 1, \\dots, n$, and targets $y_i$ (which can be composed in a vector $\\mathbf{y} = (y_1, \\dots, y_n)$):\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\min_\\alpha \\quad & \\frac{1}{2}\\boldsymbol\\alpha^\\intercal \\mathbf{Q} \\boldsymbol\\alpha - \\mathbf{e}^\\intercal \\boldsymbol\\alpha \\\\\n",
    "    \\text{subject to} \\quad & \\begin{cases}\n",
    "        & \\mathbf{y}^\\intercal\\alpha = 0 \\\\\n",
    "        & 0 \\leq \\alpha_i \\leq C \\quad \\forall\\,\\, i\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Symbols are defined as follows:\n",
    "\n",
    "* $\\boldsymbol\\alpha$ is the vector of Lagrange multipliers ($\\alpha_1, \\dots, \\alpha_n$), where $\\alpha_i$ is the Lagrange multiplier for training sample $i$\n",
    "* $\\mathbf{Q}$ is a $n$ by $n$ positive semi-definite matrix, with entries $Q_{ij} = y_i y_j \\mathbf{X}^\\intercal_i\\mathbf{X}_j$\n",
    "* $\\mathbf{e}$ is the vector of all ones (1, $\\dots$, 1)\n",
    "\n",
    "Here we have followed the notation convention of the sklearn [User Guide](https://scikit-learn.org/stable/modules/svm.html) for support vector machines.\n",
    "\n",
    "Important note: this optimisation problem only depends on the input vectors $\\mathbf{X}_i$ and $\\mathbf{X}_j$ through an inner product between them. \n",
    "\n",
    "When you work through this optimisation problem, it can be shown that the prediction of a soft-margin classifier for a test point $\\mathbf{X}_t$ is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}_t = \\text{sign}\\left(\\sum_{i\\,\\in\\,\\text{SV}}y_i\\alpha_i\\mathbf{X_i}^T\\cdot\\mathbf{X}_t + b\\right),\n",
    "\\end{equation*}\n",
    "where the sum is over the **support vectors** and the parameters $\\alpha_i$ are determined during the optimisation procedure. Each $\\alpha_i$ must be greater than 0 but less than C. The sum is only over support vectors as $\\alpha_i$ goes to 0 for all other training points during the optimisation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing this optimisation (for both soft-margin and hard-margin cases), we must compute many **inner products** between input vectors $\\mathbf{X}$. This will be important for when we come to discuss non-linear support vector machines.\n",
    "\n",
    "After manually adding some outliers to the IRIS dataset, we can see the results of a soft-margin SVC below. Note that the support vectors are all of the points on the boundary of or within the margin. \n",
    "\n",
    "<img src='Week6_plots/soft_margin_example.png' align='center' height=600>\n",
    "\n",
    "*Classification using a soft-margin. Training points are allowed into the margin, but penalised by how far into the margin they are. All support vectors lie on the edge of or in the margin.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM regression (SVR)\n",
    "\n",
    "So far we have looked at classification using SVMs, but similar principles can be applied for regression. Rather than labelling points with the sign of $\\mathbf{w}^\\intercal\\mathbf{X}_i - b$, the prediction is the value of $\\mathbf{w}^\\intercal\\mathbf{X}_i - b$. \n",
    "\n",
    "We require an additional parameter for SVR, which is denoted as $\\varepsilon$. The goal of this problem is to find some function $f(\\mathbf{X}_i)$ such that for all inputs $\\mathbf{X}_i$, the corresponding prediction $\\hat{y}_i = f(\\mathbf{X}_i)$ falls within the range $y_i - \\varepsilon \\leq \\hat{y}_i \\leq y_i + \\varepsilon$, i.e. that the prediction is no further than $\\varepsilon$ from the true value. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "\n",
    "Once again we seek to minimise $||\\mathbf{w}||^2$, but this time with different conditions. The optimisation problem can be formulated as\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w},\\,b} \\quad & \\frac{1}{2} ||\\mathbf{w}||^2 \\\\\n",
    "\\text{subject to} \\quad & \\begin{cases}\n",
    "    & y_i - (\\mathbf{w}^\\intercal\\mathbf{X}_i - b) \\leq \\varepsilon \\\\\n",
    "    & (\\mathbf{w}^\\intercal\\mathbf{X}_i - b) - y_i \\leq \\varepsilon,\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "where the two conditions are just expressing the same inequality as before, that prediction $\\hat{y}_i$ should deviate from the true value $y_i$ by no greater than $\\varepsilon$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a hidden assumption in this formulation: that for the given value of $\\varepsilon$, it is possible to find values $\\mathbf{w}$ and $b$ that approximate all of the training points ($\\mathbf{X}_i$, $y_i$) with $\\varepsilon$ precision. This is analogous to the hard-margin condition we saw for classification. \n",
    "\n",
    "In practice, we need to allow some training points to have deviation greater than $\\varepsilon$ to improve quality of fitting. \n",
    "\n",
    "Much like the soft-margin condition in classification, predictions further than $\\varepsilon$ from the training $y$ value are penalised according to how much further from the training value they are. Again the scale of this penalisation is controlled by the parameter $C$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "We define slack variables $\\xi_i$ and $\\xi_i^*$ like we did for soft-margin classification. Please note, here the $^*$ denotes two different sets of coefficients corresponding to above and below the prediction region, **not** a complex conjugation. This results in the following optimisation problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w},\\,b,\\,\\xi,\\,\\xi^*} \\quad & \\frac{1}{2}||\\mathbf{w}||^2 + C\\sum_{i = 1}^n(\\xi_i + \\xi_i^*) \\\\\n",
    "\\text{subject to} \\quad & \\begin{cases}\n",
    "    & y_i - (\\mathbf{w}^\\intercal\\mathbf{X}_i - b) &\\leq& \\varepsilon + \\xi_i \\\\\n",
    "    & (\\mathbf{w}^\\intercal\\mathbf{X}_i - b) - y_i &\\leq& \\varepsilon + \\xi_i^* \\\\\n",
    "    & \\xi_i,\\,\\xi_i^* &\\geq& 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Much like in soft-margin classification, the parameter $C$ controls how much deviation greater that $\\varepsilon$ is penalised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows an example of SVR. The dashed lines indicate the $\\varepsilon$ region where we do not care if the model prediction is not accurate. Note: see how the support vectors in this case are the vectors **outside** of the margin, rather than those inside. This is one of the key differences between support vector regression and support vector classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Week6_plots/svr_pred_plot.png' align='center' height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The optimisation procedure for SVR is broadly the same as for SVM, with the exception that we now have different constraints for errors above and below the hyperplane, quantified by $\\xi_i$ and $\\xi_i^*$ respectively. As a result, we have two sets of Lagrange multipliers and the problem looks much more horrible.\n",
    "\n",
    "For $n$ training inputs $\\mathbf{X}_i$ with $p$ features, and corresponding targets $y_i$ (which can be collected in a vector $\\mathbf{y} \\in \\mathbb{R}^n$), the optimisation problem is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\boldsymbol\\alpha, \\boldsymbol\\alpha^* } \\quad & \\frac{1}{2}(\\boldsymbol\\alpha - \\boldsymbol\\alpha^* )^\\intercal \\mathbf{Q}^* (\\boldsymbol\\alpha - \\boldsymbol\\alpha^* ) + \\varepsilon \\mathbf{e}^\\intercal(\\boldsymbol\\alpha + \\boldsymbol\\alpha^* ) - y^\\intercal(\\boldsymbol\\alpha - \\boldsymbol\\alpha^* ) \\\\\n",
    "\\text{subject to} \\quad & \\begin{cases}\n",
    "    & \\mathbf{e}^\\intercal(\\boldsymbol\\alpha - \\boldsymbol\\alpha^*) = 0 \\\\\n",
    "    & 0 \\leq \\boldsymbol\\alpha_i, \\boldsymbol\\alpha^*_i \\leq C \\quad i = 1, \\dots, n\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where the symbols are defined as follows:\n",
    "* $\\boldsymbol\\alpha = (\\alpha_1, \\dots, \\alpha_n)$ : the Lagrange multipliers for the $\\xi_i$ constraint\n",
    "* $\\boldsymbol\\alpha^* = (\\alpha^*_1, \\dots, \\alpha^*_n)$: the Lagrange multipliers for the $\\xi^*_i$ constraint\n",
    "* $\\mathbf{Q}^*$ is an $n$ by $n$ positive semidefinite matrix, with entries $Q^*_{ij} = \\mathbf{X}_i^\\intercal \\mathbf{X}_j$. \n",
    "* $\\mathbf{e}$ is the vector of ones\n",
    "\n",
    "As before we have adopted the same notation as the sklearn [User Guide](https://scikit-learn.org/stable/modules/svm.html), with the exception of denoting the matrix $\\mathbf{Q}$ as $\\mathbf{Q}^*$ to avoid confusion with the classification formulation. This does not refer to a complex conjugate. Similarly, $\\boldsymbol\\alpha^*$ is not the complex conjugate of $\\boldsymbol\\alpha$ but just denotes a different set of Lagrange multipliers, corresponding to the slack variables $\\xi_i^*$. \n",
    "\n",
    "As we saw with classification, the only dependence on the training inputs for the optimisation problem is on the inner product $\\mathbf{X}_i^\\intercal\\mathbf{X}_j$. \n",
    "\n",
    "Finally, after determination of the coefficients $\\boldsymbol\\alpha$ and $\\boldsymbol\\alpha^*$ the prediction for test input $\\mathbf{X}_t$ is given as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}_t = \\sum_{i\\in\\text{SV}}(\\alpha_i - \\alpha_i^*)\\mathbf{X}_i^\\intercal\\mathbf{X}_t + b\n",
    "\\end{equation*}\n",
    "\n",
    "where once again the sum is only over the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in classification, this optimisation also only depends on the training inputs through their inner product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we have introduced the formalism of support vector machines for linear problems, including:\n",
    "\n",
    "* basic principles of how SVMs work\n",
    "* details of SVM classification and regression\n",
    "* descriptions of soft-margin and hard-margin cases\n",
    "\n",
    "In the following section, you will work with a toy data to see how sklearn can be used for support vector regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<a name=\"section-2\"></a>\n",
    "\n",
    "## Section Two: A toy model with SVR [^](#outline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have seen the mathematics of the Support Vector Machine, we are going to try using this algorithm. Like we did for the k-Nearest Neighbours last week, we will try applying Support Vector Regression to so-called toy data. In this example we will focus on a linear regresison task, and you will have the opportunity to try a nonlinear task in Exercise Two. We can generate toy data for regression using the ```make_regression``` function from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features = 1, n_informative = 1, noise = 15, random_state = 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates linear data, with Gaussian noise added. Now we can visualise the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel(r'$\\mathbf{X}$')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this follows a linear trend, with some variation about a linear line. We should be able to fit this using a support vector machine, with a linear kernel.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like last week, we need to separate our data into training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for SVR is similar to the syntax we have seen for other ML algorithms. \n",
    "\n",
    "Note: sklearn has a couple different implementations of SVR for different cases. For a linear regression task, it is best to use ```LinearSVR```, rather than ```SVR``` with ```kernel = 'linear'```. This is due to the specific implementation of these functions: ```LinearSVR``` **only** implements a linear kernel, but for linear regression tasks is much more efficient than using ```SVR```. Beyond the additional ```kernel``` parameter for ```SVR```, the syntax is otherwise the same. We will see more about kernels in Section 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVR(epsilon=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVR</label><div class=\"sk-toggleable__content\"><pre>LinearSVR(epsilon=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVR(epsilon=10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "C = 1.0\n",
    "epsilon = 10\n",
    "\n",
    "toy_svr = LinearSVR(epsilon = epsilon, C = C)\n",
    "\n",
    "toy_svr.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of support vector regression, we need a new metric. A common choice to evaluate a general regression model is the **root-mean squared error (RMSE)**:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^N (y_i - \\hat{y}_i)^2},\n",
    "\\end{equation*}\n",
    "where $y_i$ denotes the true value of $y$ for input $\\mathbf{X}_i$, $\\hat{y}_i$ denotes the SVR prediction for input $\\mathbf{X}_i$, and N is the total number of test samples.\n",
    "\n",
    "We can make a prediction on our test data, and then calculate the RMSE to see how good the model is. A smaller value is better.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.680704838623416\n"
     ]
    }
   ],
   "source": [
    "y_pred = toy_svr.predict(X_test)\n",
    "\n",
    "rmse = (((y_test - y_pred)**2).sum()/len(y_test))**0.5\n",
    "\n",
    "print(rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like for classification accuracy, sklearn implmenets the root-mean squared error in an easy function called ```mean_squared_error```. To calculate the root-mean squared error, we pass the argument ```squared = False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.680704838623416"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, y_pred, squared = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we want to measure the performance using the cross validation score. An important detail of ```cross_val_score``` in sklearn is that it implements a fixed set of scorers, or used the default scorer of the algorithm as implemented in sklearn. For SVR, this is a metric known as the **coefficient of determination** (which you will see next week [MAYBE]?), but for consistency we will continue to use the root-mean squared error. This requires manual definition of an RMSE scorer, which we then pass to ```cross_val_score```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.91576055, 15.0656951 , 16.38859305, 17.45295476, 24.8768774 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared = False)\n",
    "\n",
    "cross_val_score(toy_svr, X_train, y_train, scoring=rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is useful to visualise the prediction of the algorithm, alongisde the $\\varepsilon$ band within which we don't care about errors made on prediction. Here, the solid line is the prediction of the SVR model, and the two dashed lines indicate the $\\varepsilon$ region in which the model doesn't care about the difference between the prediction and the true data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_range = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1,1)\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_range, toy_svr.predict(X_range), color='black')\n",
    "plt.plot(X_range, toy_svr.predict(X_range) + epsilon, color='black', ls = '--')\n",
    "plt.plot(X_range, toy_svr.predict(X_range) - epsilon, color='black', ls ='--')\n",
    "\n",
    "plt.xlabel(r'$\\mathbf{X}$')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Now you have the steps to build, train and test an SVR model, try varying the hyperparameters and try to find the best results you can. Hyperparameters to try varying include:\n",
    "* the regularisation parameter C\n",
    "* epsilon, the scale within which prediction errors are not penalised\n",
    "\n",
    "There are others, take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) to see what else you can vary. Make sure to record whatever you do vary!\n",
    "\n",
    "* Record your test data performance and present the information. \n",
    "* Plot the prediction of your best performing result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "\n",
    "Why did you choose these hyperparameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit an SVR model to your toy data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your data; calculate test set performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your best performing model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, you have worked with a toy linear model and SVR:\n",
    "\n",
    "* introduction to syntax of LinearSVR\n",
    "* the root-mean squared error metric\n",
    "\n",
    "The following section will discuss how we can tackle non-linear problems using kernels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<a name=\"section-3\"></a>\n",
    "\n",
    "## Section Three: Non-linear Support Vector Machines [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have seen for linear problems that support vector machines aim to find a hyperplane that describes the data, with different applications for classification and regression. For a hyperplane defined by a function $f(\\mathbf{X})$, prediction is defined as follows:\n",
    "\n",
    "* classification:  $\\hat{y}_i = \\text{sign}(f(\\mathbf{X_i}))$\n",
    "* regression: $\\hat{y}_i = f(\\mathbf{X}_i)$\n",
    "\n",
    "However, we have up to this point restricted our functions $f(\\mathbf{X})$ to purely linear functions, taking $f(\\mathbf{X}) = \\mathbf{w}^\\intercal\\mathbf{X} - b$. This is clearly not sufficient for a general problem that can be non-linear; for example, a classification with classes that are not linearly separable, or a regression problem that is not well modelled by a linear model. Instead, to accurately separate two classes, or solve the regression problem, we may need a more generic **hypersurface**, rather than a linear hyperplane.\n",
    "\n",
    "To be able to apply this algorithm to non-linear data, we need to introduce non-linearities. We can do this by applying a **transformation** to $\\mathbf{X}$ to transform to a different feature space in which the data is linear/linearly separable. We can represent this transformation mathematically:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi(\\mathbf{X}_i) : \\mathcal{X} \\to \\mathcal{F}\n",
    "\\end{equation*}\n",
    "\n",
    "* $\\phi(\\mathbf{X}_i$) is the transformation applied to the training input $\\mathbf{X}_i$\n",
    "* $\\mathcal{X}$ denotes the space of the original training inputs $\\mathbf{X}_i$; for $p$ input features, this is equivalent to $\\mathbb{R}^p$\n",
    "* $\\mathcal{F}$ denotes the new feature space after the transformation\n",
    "\n",
    "We will first show how this can allow us to separate classes that are not linearly separable in the original feature space $\\mathcal{X}$, then we will discuss some issues with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding additional features\n",
    "\n",
    "It is possible to introduce non-linearities to linear models by adding additional features to your inputs $\\mathbf{X}$. This can be done for both classification and regression. For example, consider the classification problem below; the two classes are not linearly separable, but intuitively you can imagine a circular curve separating the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=Week6_plots/nonlinear_example_final.png align='center' height=600>\n",
    "\n",
    "*Example classification problem where the two classes are not linearly separable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we cannot draw a single straight line that separates the two classes. This means that this data is not linearly separable. \n",
    "\n",
    "To overcome this, we can add additional features that are constructed from the existing features. In this case we could define a new feature that is the sum of the square of Feature 1 and the square of Feature 2, in analogy with a radius. Class 1 will therefore have higher values of this new feature than class 2. This can be seen in the figure below. The two classes are linearly separable in Feature 1 and Feature 3.\n",
    "\n",
    "<img src=Week6_plots/polynomial_feature_example_final.png align='center' height=600>\n",
    "\n",
    "*Now we have added a new feature to the previous dataset, the two classes are now linearly separable.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for data without a linear hypersurface in $n - 1$ dimensions, we can find a number of dimensions $m$ in which a linear hypersurface of dimensions $m - 1$ can be constructed that well-models the data, for classification or regression.\n",
    "\n",
    "However, this is not an ideal solution, because:\n",
    "\n",
    "* SVM learning relies on computing a large number of **inner products**\n",
    "\n",
    "* By adding more features to the dataset, the number of computations required increases greatly\n",
    "\n",
    "* For large datasets, this becomes computationally infeasible\n",
    "\n",
    "* You have to come up with the new features yourself; in some cases, this is straightforward, but in general it is not easy\n",
    "\n",
    "\n",
    "\n",
    "In general, it is not feasible to just add more features due to the increased computational complexity. How can we get around this?\n",
    "\n",
    "### Kernel tricks\n",
    "\n",
    "We know that our optimisation problem depends only on inputs through inner products. When we transform into the higher dimensional feature space, these inner products are now in the higher dimensional space rather than the original space We still only care about the result of the inner product; if we can somehow find the result of the inner product in the higher dimensional space from the inner product in the original space, we can save a lot of time. If we can find some function $k(\\mathbf{X}_i,\\mathbf{X}_j) = \\phi(\\mathbf{X}_i)^\\intercal\\cdot\\phi(\\mathbf{X}_j)$, we can just compute the value of this function. This approach is referred to as a **kernel trick**. The function $k(\\mathbf{X}_i,\\mathbf{X}_j)$ is referred to as a **kernel function**, or just a **kernel**.\n",
    "\n",
    "In fact, we find that for a lot of common transformations (e.g. adding polynomial features), we can define functions $k(\\mathbf{X}_i, \\mathbf{X}_j)$ that are **only** dependent on calculations in the original space. A few common kernel functions include:\n",
    "\n",
    "* Polynomial: $k(\\mathbf{X}_i,\\mathbf{X}_j) = (\\mathbf{X}_i^\\intercal \\cdot \\mathbf{X}_j + r)^d$, where $d$ is the order of the polynomial and $r$ is a bias parameter. $d$ = 1 corresponds to the linear case we have seen already.\n",
    "\n",
    "* Gaussian radial basis function: $k(\\mathbf{X}_i,\\mathbf{X}_j) = \\exp\\left(-\\gamma||\\mathbf{X}_i - \\mathbf{X}_j||^2\\right)$, where $\\gamma$ is a scale parameter and $\\gamma$ > 0\n",
    "\n",
    "* Sigmoid function: $k(\\mathbf{X}_i,\\mathbf{X}_j) = \\tanh(\\gamma\\mathbf{X}_i^\\intercal\\cdot\\mathbf{X}_j + r)$, where $\\gamma$ and $r$ are scale and bias parameters respectively\n",
    "\n",
    "These are the fundamental basis of doing any non-linear classification or regression using SVM and you will have seen ```kernel``` as an argument for the ```SVC``` you saw last week. \n",
    "\n",
    "The figure below shows the results of applying a polynomial kernel (order 2) SVM to the circular data we saw above. Now that we are using a non-linear kernel, we can easily find a curve which we can use to discriminate between the two classes. The same analysis can be applied to higher dimensional problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=Week6_plots/nonlinear_svm_decision_boundary.png align='center' height=600>\n",
    "\n",
    "*Prediction of a non-linear SVC model with a polynomial kernel, order 2. The shaded regions indicate the class that would be predicted in that region.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same principle is equally applicable to regression. For example, we can see how using a linear kernel vs a polynomial kernel performs on some quadratic data with Gaussian noise. This can be seen in the Figure below. The dotted lines indicate the $\\varepsilon$ regions where we don't care about prediction mistakes. The linear kernel clearly does not work, but modelling using a polynomial kernel with degree 2 works well.\n",
    "\n",
    "<img src=Week6_plots/linear_vs_polynomial_kernel_SVR.png align=\"center\" height=500>\n",
    "\n",
    "*SVR using linear and polynomial kernels, for quadratic data with Gaussian noise. Clearly the linear model does not fit, but the polynomial kernel performs well.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we have covered how we can tackle non-linear problems using SVMs, including:\n",
    "\n",
    "* adding additional input features\n",
    "* transforming inputs to a higher dimensional input space\n",
    "* kernel tricks\n",
    "\n",
    "The following section will introduce the dataset we will be using for the remainder of this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Section Four: The California Housing dataset [^](#outline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have experimented with SVR on toy data, we will now introduce the dataset you will work with for the remainder of this week. We will work with another dataset that is built into sklearn, the California housing dataset. This datset is derived from the 1990 U.S. census, which comprises house prices grouped by block group (the smallest geographical unit used by the US Census Bureau, typically 600-3000 people) and 8 features for each sample. \n",
    "\n",
    "Further details can be found in the sklearn [User Guide](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset). You are encouraged to take a look at this, as it explains exactly what each of the input features are. Much like the IRIS dataset you saw last week, it is easy to import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target for this dataset is the median house value in a given block group. As always, it is important to visualise our dataset before we start with any ML. You can see the ```scatter_matrix``` below. As a quick refresher on ```scatter_matrix```, the plots on the line from the top left to the bottom right are the histograms of the relevant quantities, whereas the rest of the plots are scatter plots of each quantity against the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=Week6_plots/california_h_scatter_matrix.png align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly more complex than the IRIS dataset, due to the increased number of input features. The scatter point color denotes the value of the target, the median house value (in units of $100,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, you have had a brief introduction to the California housing dataset. \n",
    "\n",
    "You will use this dataset and a toy non-linear dataset to practise using SVR for the remainder of this week. \n",
    "\n",
    "The following section covers the exercises for you to work on yourselves this week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<a name=\"section-5\"></a>\n",
    "\n",
    "## Section Five: Exercises [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear toy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you have seen a toy model with linear data before, here we will define a non-linear dataset for you to practise on, using the ```SVR``` class in sklearn. First, we will define a function to generate random data from a randomly determined polynomial. The X values are drawn from a uniform distribution, with a maximum value of -5 and a minimum value of +5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial(n_points):\n",
    "    degree = np.random.randint(2,10)\n",
    "    coeffs = np.random.uniform(-3,3,size = degree + 1)\n",
    "    X = np.random.uniform(-5,5,n_points)\n",
    "    y = np.vstack([coeffs[i]*X**i for i in range(1,degree+1)]).sum(axis = 0) + coeffs[0] + np.random.normal(loc = 0, scale = 5, size = n_points)\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_polynomial(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Train a support vector machine to model this data. Try to find the hyperparameters to give the best performance on the test data. Remember, steps go as follows:\n",
    "\n",
    "* Split data into test and training sets\n",
    "* Define your hyperparameters\n",
    "* Define your model\n",
    "* Fit the model to your training data\n",
    "* Predict on the test data and calculate the RMSE between the test data and prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data & calculate RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Write a couple sentences explaining why you have chosen the hyperparameters you have chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California housing dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin using SVR with the California housing dataset, we want to get more familiar with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise Two\n",
    "\n",
    "Visualise the California housing dataset. You could do this using ```scatter_matrix```, for example, or any method of your choice.\n",
    "\n",
    "Write short answers to the following questions:\n",
    "\n",
    "* Do any features appear to correlate well with the target?\n",
    "* Any first ideas as to what kind of model might work with this data?\n",
    "* Is the target approximately linear in any of the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR with the California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise Three\n",
    "\n",
    "Now you have practised using SVR for a non-linear dataset with 1 input feature, you will apply it to the California housing dataset.\n",
    "\n",
    "* Prepare the California housing dataset - split into test & training sets\n",
    "* Define your hyperparameters\n",
    "* Prepare and train the model\n",
    "* Evaluate the model performance on the test data\n",
    "\n",
    "Try varying the hyperparameters to get the best performance you can. Hyperparameters to vary include:\n",
    "\n",
    "* The regularisation parameter ```C```\n",
    "* The deviation below which errors are ignored ```epsilon``` ($\\varepsilon in the theory discussion earlier)\n",
    "* The kernel\n",
    "\n",
    "Note each kernel has specific parameters that you can also vary. See the [User Guide](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) for more details.\n",
    "\n",
    "Make sure you record all hyperparameters you use for a given result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Explain your final choice of kernel and other hyperparameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
