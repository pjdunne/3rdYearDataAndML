{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Outline <a id='outline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Section One: Introduction to convolutional neural networks](#section-1)\n",
    "1. [Section Two: Convolutional neural networks in PyTorch](#section-2)\n",
    "1. [Section Three: A convolutional neural network example with Fashion MNIST](#section-3)\n",
    "1. [Section Four: The CIFAR-10 dataset](#section-4)\n",
    "1. [Section Five: Exercises](#section-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Section One: Introduction to convolutional neural networks [^](#outline) <a id='section-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have tackled problems we can solve with linear layers and nonlinear activation functions. Let us consider an image classification problem, for example using the MNIST dataset (which is a dataset of 28 by 28 grayscale images). \n",
    "\n",
    "If we consider a binary classification problem such that we have a single output, we flatten our 28 by 28 image input into a 784 length input vector, and we construct a fully connected network with a single hidden layer of 512 neurons, we have a total number of weights of $784 \\times 512 + 512 \\times 1 = 401920$. For just one hidden layer we already have more 400000 parameters! As we construct deeper networks, our number of parameters quickly grows out of hand for any image classification problem. \n",
    "\n",
    "If we now consider the same network construction for a full HD image that is 1920 by 1080, we find we have over a billion parameters for this network. If we consider a colour image,  Clearly, it will very quickly become unfeasible to train a standard neural network for image classification tasks. Because of how many parameters we have, we are likely to overfit when we train such a network. How can we solve this problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at an image, a given pixel is likely to share structure with other pixels near it in the image. We can use this to inform how we structure our model; we want to somehow use this shared **local** structure to develop a good classifier. \n",
    "\n",
    "While our standard neural networks are made of linear layers and activation functions, convolutional neural networks add a new type of layer: convolutional layers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layers\n",
    "\n",
    "You will be familiar with convolutions from previous courses on Fourier transforms, but we will briefly review the fundamental concepts before we discuss how they are applied in machine learning.\n",
    "\n",
    "A convolution between two functions produces a third function that expresses how the shape of one of the functions is modified by the other. The convolution of two functions $f$ and $g$ can be written as \n",
    "\n",
    "\\begin{equation*}\n",
    "(f * g)(t) = \\int_{-\\infty}^\\infty f(t - \\tau) g(\\tau) d\\tau,\n",
    "\\end{equation*}\n",
    "where $\\tau$ is a dummy variable introduced for the integration. We can see this as \"sliding\" the function $f(t - \\tau)$ over the function $g(t)$ as we vary $\\tau$ in the integration. \n",
    "\n",
    "If instead our functions $f$ and $g$ are functions of discrete variables, we can write this expression as a sum instead of an integral:\n",
    "\n",
    "\\begin{equation*}\n",
    "(f * g)(t) = \\sum_{\\tau = -\\infty}^\\infty f(t - \\tau) g(\\tau)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply convolutions in convolutional neural networks, we refer to the function $g(\\tau)$ as the input, and the function $f(t - \\tau)$ as the **kernel**. The output function $(f * g)(t)$ can be referred to as the **feature map**. Typically, our input is some multidimensional array of data (for example, a colour image with 3 colour channels corresponding to red, green and blue) and the kernel is some multidimensional array of parameters which are learned in the training process. \n",
    "\n",
    "Because our inputs are in general finite numerical arrays, we can instead of taking the sum to $-\\infty$ and $\\infty$ we can instead sum over a finite number of array elements. We also often use convolutions over more than one axis at a time. For example, for a two-dimensional image $I$ as the input and a two-dimensional kernel $K$, we can write the convolution as\n",
    "\n",
    "\\begin{equation*}\n",
    "S(i, j) = (I * K)(i, j) = \\sum_m \\sum_n I(m, n) K(i - m, j - n)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it simply when we apply a kernel of size $N \\times M$:\n",
    "\n",
    "* We take all of the pixels in an $N \\times M$ grid around a point in the image and performs a weighted sum, where the kernel contains the weights we use\n",
    "<br>\n",
    "\n",
    "* Then, move the kernel one pixel over and take another weighted sum (with the same weights) of the new set of pixels. \n",
    "<br>\n",
    "\n",
    "* You repeat this process for the whole image \n",
    "\n",
    "The resulting array is called the feature map and effectively encodes relationships between pixels that are close together, depending on the parameters in the kernel. \n",
    "\n",
    "The figure below is an animation showing how we build a feature map from a $4 \\times 4$ input and a $2 \\times 3$ kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Week10_plots/conv.gif' align='center' width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features of convolutional layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers learn from data by changing the parameters kept in the kernel. This results in a few key features for this type of network:\n",
    "\n",
    "1. Sparse connectivity\n",
    "1. Re-use of parameters\n",
    "1. Translational invariance\n",
    "\n",
    "We will discuss each of these in turn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse connectivity\n",
    "\n",
    "Consider a $m \\times n$ linear layer. All $m$ input values are connected to all $n$ output values, resulting in an array of weights you have to learn that is $m \\times n$ in size. \n",
    "\n",
    "In contrast, for a convolutional network, the weights array we learn in a given layer is just the size of the kernel; e.g. for a kernel of shape $i \\times j$, even if we have $m \\gt i,\\, j$ inputs, we only have to learn $i \\times j$ parameters for that kernel. This greatly reduces the number of  weights we have to learn, to get the same sized input/output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use of parameters\n",
    "\n",
    "In a convolutional layer, all of our parameters are just in the kernel. As a result, we *only* need to learn the parameters in the kernel, rather than having to learn a parameter for every input, or indeed every connection between every input and every neuron in the following layer, like we have to do for a standard linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translational invariance\n",
    "\n",
    "If we train a standard neural network on an image dataset, the neurons in individual layers learn to respond to specific pixels in the image. For example, consider an neural network to identify where a dog is in an image. If we just train it with pictures of dogs in the lower right of the image, as soon as we present a data point with the dog in e.g. the upper left, the model will perform poorly.\n",
    "\n",
    "In contrast, because for a convolutional neural network the weights are all stored in the kernel, you do not learn on specific pixels and instead extract relationships between adjacent pixels. This means you can pick up individual patterns regardless of where they are in the input, meaning your network response has translational invariance. \n",
    "\n",
    "For example, say we have a kernel that is good at locating a dog in an image. Because all the weights are in the kernel, if we shift the location of a dog in an image all we do is shift the corresponding output by the same amount. We will be equally good at detecting the dog regardless of where it is in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the size of CNNs\n",
    "\n",
    "While up to this point we have just discussed how CNNs are more effective that traditional neural networks for computer vision, we do need to be aware of the size of our CNNs. After we apply a given kernel, depending on the size of the kernel and the size of the input, we can get an output of the same size or smaller than our input. We can also increase the size relative to the input, by using more than one kernel. \n",
    "\n",
    "Typically, if there are more specific features we want to try to extract, we will use more kernels. For example, there are two different kernels for extracting vertical and horizontal edges from an image we saw earlier called the [Sobel operators](https://en.wikipedia.org/wiki/Sobel_operator). However, the more kernels we use, the more output neurons we end up with after our convolutional layer. \n",
    "\n",
    "If we are using multiple kernels every layer, we can quickly end up with much deeper layers and many more neurons than we started with, but when we come to make our final prediction we need to reduce our size down to what our output needs to be, e.g. a single number of we are predicting a single class or a single regression value. There are three main ways we can do this:\n",
    "\n",
    "1. Have a linear, fully-connected layer at the end of our network\n",
    "<br>\n",
    "\n",
    "1. Increase the **stride**\n",
    "<br>\n",
    "\n",
    "1. Use **pooling**\n",
    "\n",
    "The first of these results in many parameters that need to be trained, exactly what we wanted to avoid! So in general we won't do this. Instead, we make use of stride and pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "The stride of a convolutional layer refers to how many pixels we move our kernel across our input when we pass through a convolutional layer. If we increase the stride, we move by more pixels between applications of the convolutional kernel; e.g. if we have a stride of N, we will do one convolution, then move our kernel by N pixels before we do another convolution. This is illustrated in the animation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Week10_plots/stride.gif' width=1000>\n",
    "\n",
    "*Illustration of the effect of setting stride > 1 for a convolution. Whereas on the left we have a stride of 1 and produce an output that is 5 x 5 from the 6 x 6 input, on the right with a stride of 2 we instead produce a 3 x 3 output. We can therefore use stride to reduce the size of the output from our layer. $X_i$ denotes the $i$th input, $w_j$ denotes the $j$th weight in the kernel, and $o_k$ denotes the $k$th output from the convolutional layer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is important to note that when using stride we can lose information; for example, we can miss connections between two adjacent pixels if they are not covered by the kernel in a single step. In our example animation, any connection between $X_1$ and $X_2$ will be missed with our stride = 2 kernel. We do therefore need to be careful with when we use stride."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "So far we have just discussed how we can change the process of the convolution. However, this is not the only thing we can do; instead, we can do pooling. You can think of this a bit like reducing the resolution of our image. \n",
    "\n",
    "When pooling, we separate our input into small blocks or 'pools', e.g. 2 x 2 volumes of inputs, and applying some mathematical operation to each pool to obtain a single value for each pool. The output size is then reduced by a factor of N, where N is the number of inputs in each pool. For our 2 x 2 case, we therefore reduce the size by a factor of 4.\n",
    "\n",
    "The most common choice of operation to apply is taking the maximum, i.e. the output for a given pool is the maximum value of all of the inputs in the pool. This has the advantage that if your kernel has found a very localised feature, you will not dilute the response to that feature. A schematic illustrating max pooling can be seen in the figure below.\n",
    "\n",
    "<img src='Week10_plots/pool_schematic_final.svg' width=1000>\n",
    "\n",
    "*Schematic illustrating max pooling, using a 2 x 2 filter and a stride of 2. The resulting output is the maximum value in each pool.*\n",
    "\n",
    "Another option is so-called 'average' pooling, where you take the mean of the inputs in the pool. However, in general max pooling gives better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "When we apply a convolutional layer, we may also want to keep the output size the same as the input size. However, if our kernel is bigger than 1 x 1, this won't happen unless we use **padding**. \n",
    "\n",
    "Padding refers to adding extra values into the input around the edges, so we can place our kernel on our input enough times to produce an output of the same size. This is also important to make sure we get the most use out of inputs in the corners and on edges. The schematic below illustrates padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Padding schematic -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Section Two: Convolutional neural networks in PyTorch [^](#outline) <a id='section-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure: \n",
    "\n",
    " - Intro to torchvision (datasets, transforms)\n",
    " - Types of layer (convolutional, pooling, padding)\n",
    " - \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "# Section Three: A convolutional neural network example with Fashion MNIST [^](#outline) <a id='section-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard example exercise; FashionMNIST classification (set up single class problem? or let them do multiclass?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Section Four: The CIFAR-10 dataset[^](#outline) <a id='section-4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Section Five: Exercises <a id='section-5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard style exercises; load CIFAR-10, visualise, try N different architectures? Try adding pooling, stride, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
