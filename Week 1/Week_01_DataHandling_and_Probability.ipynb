{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Data Handling and Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<a id='outline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline <a id='outline'></a>\n",
    "1. [Data Handling using Pandas](#section-1)\n",
    "2. [Pandas with Datasets on File](#section-2)\n",
    "3. [Basics of Probability](#section-3)\n",
    "4. [Probability Distributions](#section-4)\n",
    "5. [Monte Carlo methods of Data Generation](#section-5)\n",
    "6. [Exercises](#section-7)\n",
    "7. [Further Exercises](#moreexercises)\n",
    "\n",
    "We strongly recommend you find your copy of notes from the PHYS40005 Statistics and Measurement notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section One: Data Handling using Pandas [^](#outline) <a id='section-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Pandas**](https://pandas.pydata.org/) is a well known and popular Python library for data manipulation, analysis and display. You will find a lot of help for pandas on the web. \n",
    "\n",
    "Pandas has a particular way of working that is distinct from traditional computer languages such as python but which echoes that used in other systems used for data manipulation and statistics, such as the [`R` language](https://en.wikipedia.org/wiki/R_(programming_language)). In particular, in pandas, we deal with objects known as \"Series\" and \"**DataFrames**\". \n",
    "\n",
    "* A Series is a labelled collection of data, similar to a dictionary. Each element can have a different data type. \n",
    "* A DataFrame is a 2D labelled data structure, where each column can have a different data type. Each row of a DataFrame can be thought of as a Series (and indeed, if you select a single row of a DataFrame it is returned as a Series). \n",
    "\n",
    "DataFrames are more useful and more commonly used, so we will focus our discussion just on DataFrames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "\n",
    "A DataFrame is a tabular data structure, similar to an Excel spreadsheet. The data is organised by rows and columns. These are labelled:\n",
    "* Rows are labelled by an **index**\n",
    "* Columns are labelled by **headers**\n",
    "\n",
    "Data can be accessed according to the headers or the index. Instead of just being numerical indices, like you have with e.g. numpy arrays, these can be descriptive strings. As a result, you can access data in a descriptive way. \n",
    "\n",
    "The cell below shows a couple examples of how you can construct DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Method 1: Set data as dictionary structure. Data formatted in columns\n",
    "\n",
    "data={'Name':[\"Rex\",\"Bruno\",\"Biffa\",\"Queeny\", \"Bob\", \"Sheiba\", \"Crusoe\"],\n",
    "     'Breed':[\"bulldog\",\"labrador\",\"doberman\",\"poodle\", \"pug\", \"labrador\", \"scotty\"],\n",
    "     'Age':[2,4,12,0.5, 7, 10, 7]}\n",
    "\n",
    "dogs=pd.DataFrame(data)\n",
    "\n",
    "display(dogs)\n",
    "\n",
    "# Method 2: Splitting Headers and data. Data formatted in rows\n",
    "\n",
    "d=[[\"Rex\",\"bulldog\",2],\n",
    "    [\"Bruno\",\"labrador\",4],\n",
    "    [\"Biffa\", \"doberman\", 12],\n",
    "    [\"Queeny\",\"poodle\", 0.5],\n",
    "    [\"Bob\", \"pug\", 7],\n",
    "    [\"Sheiba\", \"labrador\", 10],\n",
    "    [\"Crusoe\",\"scotty\",7]]\n",
    "\n",
    "Headers=['Name', 'Breed', 'Age']\n",
    "\n",
    "dogs2=pd.DataFrame(data=d,columns=Headers)\n",
    "\n",
    "display(dogs2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the index. We can select data based on the header (`display` gives nicer formatting than `print`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs.Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More generally you can use `[]` to select data, a bit like you do for slicing numpy arrays. \n",
    "\n",
    "* Passing a single label in `[]` to a DataFrame selects a column; `df['A']` is equivalent to `df.A`.\n",
    "* Passing a slice in `[]` selects matching rows, e.g. `df[2:4]` gives you the rows matching the slice 2:4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dogs['Breed'])\n",
    "\n",
    "display(dogs[2:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to access data is through the `loc` and `iloc` methods, standing for location and integer location respectively. \n",
    "\n",
    "* `loc` takes headers or indices **as seen in the DataFrame**, e.g. if we had a non-numeric index we could use the labels in the index\n",
    "* `iloc` takes numeric indices, like numpy arrays\n",
    "\n",
    "For both methods, the syntax is the same:\n",
    "\n",
    "* First argument specifies the desired row(s)\n",
    "* Second argument specifices the desired column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dogs.loc[0,:]) \n",
    "\n",
    "display(dogs.loc[:,'Breed'])\n",
    "\n",
    "display(dogs.loc[2:4,:])\n",
    "\n",
    "display(dogs.loc[:,['Breed','Age']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a new column or row to a DataFrame by indexing by the desired column name or row index, and setting it equal to a list of values equal to the number of rows/columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs.loc[:,\"Length\"]=[50,100,105,85, 40,100,80]\n",
    "dogs.loc[len(dogs)] = ['Daisy','papillon',3.5, 35]\n",
    "display(dogs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add new rows to a DataFrame, combining pandas objects using `concat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.DataFrame({'Name':'Ham','Breed':'shiba inu','Age':10,'Length':40},index = [len(dogs)])\n",
    "\n",
    "dogs = pd.concat([dogs, new_row])\n",
    "display(dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our default index here is just a numerical index; we can change it to something more useful, like the Name column. This way, someone can access all the data about a given dog by querying that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = dogs.set_index('Name')\n",
    "\n",
    "display(dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create columns that are combinations of other columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs[\"combination\"]=dogs.Age*dogs.Length\n",
    "display(dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation of DataFrames\n",
    "\n",
    "There are many ways you can sort and filter DataFrames to find the information you want. You can do simple indexing like you might with numpy arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dogs[dogs.Age > 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a lot of information about how you can slice and query for data, as well as adding new data to your DataFrames, in the pandas [User Guide](https://pandas.pydata.org/docs/user_guide/index.html). A good place to get started is the [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Using the dogs DataFrame we defined in Section Three, try the following operations:\n",
    "\n",
    "* Define a new column for with the header \"Weight\"; make up some data of your own to put in this column\n",
    "* Make up some data for another dog, and add it to the DataFrame. Remember: we have set the index of the DataFrame to be the name, so the DataFrame you define for the new dog must have its name as its index.\n",
    "* Define a column that is the weight per unit length\n",
    "* Calculate the mean and standard deviation of each of the numeric quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for Weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data for another dog\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a weight per unit length column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation for each of the numeric quantities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we have covered some basics of `pandas`, a very useful data library in Python. This has included:\n",
    "\n",
    "* Defining a DataFrame\n",
    "* Printing the contents of a DataFrame in a nice way\n",
    "* Finding slices of a DataFrame\n",
    "* Adding new rows and columns to a DataFrame\n",
    "\n",
    "The following section will some more advanced techniques using pandas, including reading and writing DataFrames to and from files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> \n",
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Two: Pandas with Datasets on File [^](#outline) <a id='section-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have dealt with a very simple DataFrame that we have defined ourselves, to demonstrate how you can add data to an existing DataFrame. In reality, the main use of pandas is for big DataFrames, and handling a lot of data at once. Typically these will be stored in a file. You can save pandas DataFrames in a number of different ways, such as:\n",
    "* Excel spreadsheets\n",
    "* csv files\n",
    "* JSON files\n",
    "\n",
    "and more. The full list of input/output functions can be found in [the documentation](https://pandas.pydata.org/docs/reference/io.html), but here we will deal with a file saved in an .csv file. First, we load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data = pd.read_csv('concrete_data.csv')\n",
    "display(concrete_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset has more than 1000 entries, with 9 columns per entry. This dataset is a sample machine learning dataset from the UC Irvine Machine Learning Repository, and can be found [here](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength). It can be used to test machine learning methods to predict the compressive strength of concrete, from the age of the concrete and ingredients used to make it. \n",
    "\n",
    "Column units are as follows:\n",
    "* First 7 columns (Cement to Fine Aggregate): $\\text{kg m}^{-3}$. Note this is not density, but the weight of the relevant ingredient in a $\\text{m}^3$ mixture of that concrete sample.\n",
    "* Age: days\n",
    "* Strength: MPa\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly do plotting from Pandas, e.g. plotting the Cement content against the concrete strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.plot(x='Cement',y = 'Strength',kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can plot a histogram of the cement content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.hist('Cement',bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very basic plots that you can produce quickly to explore a dataset. Later in this workbook you will see how we can produce clear and professional plots. \n",
    "\n",
    "There are lots of options for visualisation from Pandas DataFrames directly that we don't have the time to cover in detail here, see [the documentation](https://pandas.pydata.org/docs/user_guide/visualization.html) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Use some of the pandas plotting methods to visualise some of the data from our dogs DataFrame in the previous section. Try:\n",
    "\n",
    "* Scatter plot of one quantity against another\n",
    "* Histogram of one of the quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "End of exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we wanted to look for concrete samples with a Water content of greater than 100 $\\text{kg m}^{-3}$, but an age of less than 100 days. We can index in a boolean fashion, as described before, or use `query`. `query` performs the same actions as indexing with a boolean condition, but is slightly faster due to the specific implementation. This is only generally noticeable for large DataFrames, but it is good practise in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(concrete_data.query('Water > 100 and Age < 100'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, `query` is very powerful and a useful tool for filtering and selecting data. It returns a DataFrame consisting of the rows of the original DataFrame that satisfy the boolean condition(s) passed. Read more in [the documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Using `query`, find concrete samples less than 30 days old and plot a histogram of the Strength, using 20 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your query\n",
    "\n",
    "\n",
    "# Your histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "End of exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other very useful way to filter data is `groupby`\n",
    "* You can group by values in one column\n",
    "* Useful for datasets with categorical or discrete data in at least one column\n",
    "* Useful to process different data sets in one DataFrame quickly\n",
    "\n",
    "For example, with our concrete data we could group by the age and find the mean and standard deviation of the strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = concrete_data.groupby('Age')['Strength'].mean()\n",
    "stds = concrete_data.groupby('Age')['Strength'].std()\n",
    "\n",
    "display(means)\n",
    "display(stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupby` is especially useful when you have a complex operation that you want to apply to different groups of data separately, but still quickly. \n",
    "\n",
    "You can define your own function to use in `groupby` as well. This is beyond the scope of this course but you can find more details about `groupby` in [the documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find some useful summary statistics for each column using `describe`. This gives us the following statistics per column:\n",
    "\n",
    "* `count`: the number of entries in that column\n",
    "* `mean`: the mean value \n",
    "* `std`: the standard deviation \n",
    "* `min`: the minimum value\n",
    "* `25%`: the lower quartile value\n",
    "* `50%`: the median value\n",
    "* `75%`: the upper quartile value\n",
    "* `max`: the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a new column that is the sum of the total ingredients in each concrete sample, by summing the ingredient columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data['Ingredient Total'] = concrete_data.loc[:,['Cement','Blast Furnace Slag','Fly Ash','Water','Superplasticizer','Coarse Aggregate','Fine Aggregate']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save our updated data to a new Excel spreadsheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.to_excel('concrete_data_new.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "When we save structured data such as DataFrames or dictionaries to file, we must undergo a process called **data serialisation**. You don't in general need to know how this works, just that there are different methods. In Python, the default method is using the **pickle** module; if you ever save a dictionary to a .npy file using numpy, it implicitly uses pickle. This process is referred to as pickling the data. In several places you will be given data saved to a .npy file that is contained within a dictionary. To access this data, you will need the following call signature:\n",
    "\n",
    "`data = np.load('file/name',allow_pickle = True).item()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section we have seen some more pandas syntax, including: \n",
    "\n",
    "* loading and saving DataFrames from and to .csv files\n",
    "* querying DataFrames to use multiple Boolean conditions\n",
    "* Grouping data and finding summary statistics of each group\n",
    "* Finding total summary statistics of our dataset\n",
    "\n",
    "The following section will discuss how you can use `matplotlib` make good, clear plots of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> \n",
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Three: Basics of Probability [^](#outline)<a id='section-3'></a>\n",
    "\n",
    "This section is a recap of some basics of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is probability?\n",
    "\n",
    "You should already have an intuition as to what probability means. The fundamental mathematical formulation of probability doesn't assign a meaning to probability but starts with the three \"Kolmogorov axioms\":\n",
    "\n",
    "* The probability $P(X_i)$ that an event $X_i$ occurs should be non-negative, $P(X_i) \\geq 0$. \n",
    "* The probability $P(X_i \\text{ or } X_j)$ that either one of two distinct events $X_i$ and $X_j$ occurs is simply the sum of the probability of each individual event occuring, that is  $P(X_i \\text{ or } X_j)$ = $P(X_i)$ + $P(X_j)$\n",
    "* The sum over all probabilities, the probabilit of some event occuring, is one, $\\sum_\\Omega P(X_i)$ = 1\n",
    "\n",
    "Here $\\Omega$ denotes the set of all possible outcomes and is known as the **sample space**. For instance, here the events could be the result obtained when we roll one six-sided die, so $X_i$ is the event where the die shows the number $i$.\n",
    "\n",
    "These axioms do not place many restrictions on probability, but to do anything particularly meaningful we need to choose a specific interpretation. You will have seen the two different interpretations used in the physical sciences before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist probability\n",
    "\n",
    "**Frequentist probability** is related to the frequency with which events occur in repeated trials:\n",
    "\n",
    "* Consider an experiment with multiple possible outcomes, where one outcome $X$ is desired\n",
    "* For $N$ measurements, the desired outcome $X$ is observed $n$ times\n",
    "* The probability that any single event of the $N$ events is given by the limit of the ratio\n",
    "\n",
    "\\begin{equation*}\n",
    "P(X) = \\lim_{N\\to\\infty}\\frac{n}{N}\n",
    "\\end{equation*}\n",
    "\n",
    "There is a clear limitation to this approach: it can only be applied when the experiment can be repeated. However, this probability is independent of any biases, so long as the outcome X is agreed to be the same between different people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian probability\n",
    "\n",
    "**Bayesian probability** abandons the concept of frequency, and instead defines probability as something that can be applied to non-repeatable experiments.\n",
    "\n",
    "* Bayesian probability is based on *degree of belief* in some event $X$ occurring, i.e. how likely you think something is to happen\n",
    "* For example, if you see betting odds that say the odds of $X$ occuring are 4:1, you might assign a probability of X occurring P(X) = 1/(1 + 4) = 1/5\n",
    "\n",
    "An observer's estimation of Bayesian probability depends on the observer and the state of the time of observation. As we see the system change, an observer may update their value of $P(X)$ as they gain more information. In this way we can also see Bayesian probability as a statement of knowledge. Some other points of Bayesian probability:\n",
    "\n",
    "* This is subjective; depending on what the observer knows, they might assign a different probability to an event $X$.\n",
    "* Can be used for events where we cannot repeat the experiment, but have some intuition or guess as to what the probability is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of probability and Bayes theorem\n",
    "\n",
    "Here we will recap some simple properties of probability, and Bayes theorem.\n",
    "\n",
    "Consier now two events $A$ and $B$ both of which can happen together. For instance $A$ maybe the event where you friend arrives at the lecture wearing black shoes and $B$ might be the event where your friend is wearing a red jumper.  We define the probability of each event as $P(A)$ and $P(B)$ respectively. \n",
    "\n",
    "We need some notation:-\n",
    "* $P(A\\text{ and }B)$ denotes the probability of both event $A$ and event $B$ occuring,\n",
    "* $P(A\\text{ or }B)$ denotes the probability of either event $A$ or event $B$ occuring but not both,\n",
    "* $P(A|B)$ refers to the **conditional probability** of event $A$ occuring given that event $B$ has already occured. \n",
    "\n",
    "For instance, if our friend has called befroe they arrive in the lecture theatre and mentioned they have their red jumper on, then $P(A|B)$ tells us teh probability that you will see them wearing black shoes they arrive in the lecture theatre.\n",
    "\n",
    "Given this notation and definitions, we then have that the following conditions are true:\n",
    "\n",
    "* $P(A\\text{ or }B) = P(A) + P(B) - P(A\\text{ and }B)$,\n",
    "* $P(A\\text{ and }B) = P(A|B)P(B) = P(B|A)P(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Events\n",
    "\n",
    "It is always good to consider the very special case where two different events are **independent**. That is the probability of $B$ happening does not depend on what happens for event $A$. \n",
    "\n",
    "For instance, the probability a Geiger counter clicks (or not) at a particular time $t_A$ (event $A$) is independent of whether it clicks (or not) at a different time $t_B$ (event $B$). Radioactive decay of one nuclei happens independently of all other nuclei in a radioactive material. On the other hand, surely the probability that tennis player Novak Djokovic wins the French open in June should be influenced by his success (or not) at the Australian open in January.\n",
    "\n",
    "The case of statistical independence is often used as a simple baseline (sometimes called a *null model*) used in a comparison to help us see if our data has something \"interesting\" happening. For instance maybe it is mostly luck for a tennis player to win any one tournament, i.e. a simple roll of the tennis dice, each one independent of another.  We could use the statistical independence hypothesis to compare against actual data on results (see hypothesis testing next week but not for the answer to sports questions).    \n",
    "\n",
    "For two independent events $A$ and $B$, we know that probability of event $A$ if we know that event $B$ has occurred, that is the conditional probability $P(A|B)$ is simply $P(A|B) = P(A)$. As a result, for independent $A$ and $B$:\n",
    "* $P(A\\text{ and }B)$ = $P(A)P(B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "**Bayes theorem** is a very important theorem that allows us to update our estimation of probability of events. It relates the conditional probabilities for $A$ and $B$:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation*}\n",
    "\n",
    "Bayesian statistics makes more particular use of this theorem, for updating belief in parameters. We can think about this in terms of **hypotheses**:\n",
    "\n",
    "* Consider a hypothesis $H(\\theta = \\theta_i)$ that the parameter $\\theta$ is equal to a specific value $\\theta$\n",
    "* In Bayesian probability, $P(\\theta_i)$ represents the degree of belief in this hypothesis\n",
    "* For frequentists, $\\theta_i$ is not a random variable so cannot have a probability assigned to it\n",
    "* Bayesian probability says we can apply Bayes theorem to this hypothesis, after making a set of observations $\\mathbf{X}$ = $\\{X_0, X_1, \\cdots\\}$; this gives us\n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\theta_i|\\mathbf{X}) = P(\\mathbf{X}|\\theta_i)\\frac{P(\\theta_i)}{P(\\mathbf{X})},\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "* $P(\\mathbf{X})$ is the probability of observing the data, summed over all possible values of $\\theta$. This is known as the **marginal probability** or the **evidence**.\n",
    "* $P(\\theta_i)$ is your initial probability for the parameter $\\theta$ = $\\theta_i$, the belief in your hypothesis. This is known as your **prior**.\n",
    "* $P(\\mathbf{X}|\\theta_i)$ is the probability of measuring your data, given the parameter $\\theta$ is equal to $\\theta_i$. This is known as the **likelihood**. \n",
    "* $P(\\theta_i|\\mathbf{X})$ is the probability that $\\theta$ = $\\theta_i$, given the evidence $\\mathbf{X}$. This is known as the **posterior**.\n",
    "\n",
    "Bayes theorem allows us to update our belief in a given hypothesis based on evidence. You can also use it multiple times consecutively; consider a second set of observations $\\mathbf{Y} = \\{Y_0, Y_1, \\cdots\\}$. We can calculate a new posterior using Bayes theorem again:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\theta_i|\\mathbf{X},\\mathbf{Y}) = P(\\mathbf{Y}|\\theta_i)\\frac{P(\\theta_i|\\mathbf{X})}{P(\\mathbf{Y})}\n",
    "\\end{equation*}\n",
    "\n",
    "Here you can see we have substituted our previous posterior as our prior for this calculation. In this way we can incorporate new data into our degree of confidence in our hypothesis as we obtain more data. Later in this notebook we will use Bayes theorem to solve the famous Monty Hall problem, and compare it to a frequentist probability found by running a simulation of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this section, we have reviewed some basics of probability, including:\n",
    "\n",
    "* Frequentist and Bayesian interpretations\n",
    "* Kolmogorov axioms\n",
    "* Bayes theorem\n",
    "\n",
    "In the following section, we will discuss different probability distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Four: Probability Distributions [^](#outline)<a id='section-4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In experimental physics, you will often see random variables. For instance, quantum processes are by their nature probabilistic so we need probability distributions to describe results for radioactivity or fundamental particle interactions in CERN experiments. We therefore expect to encounter random variables in what we measure. \n",
    "\n",
    "There are two variations to consider: **discrete** valued results and **continuous** values results. Counting the number of radioactive counts in 1 minute is a discrete valued measurement. Asking about the time interval between succesive counts on a Geiger counter is a continuous valued meaurement. The principles are the same for both cases but the notation and mathematics are slightly different. \n",
    "\n",
    "In practice, we can never record a continuous variable exactly. At best, our measurements have limited resolution e.g. we may only be able to time clicks to the nearest microsecond because of the frequency at which our digital kit works at. The energy and momentum of a particel at CERN may only be known to a few percent. So it is common to analyse continuous data in terms of **bins**.  That is we might record the number of events at CERN that have energy between 125GeV and 125.5GeV, one \"bin\" in energy if our energy resolution is 0.1 GeV. Now the energy of each particle is a discrete value as we are working in terms of the energy bins. So even with continuous variables we may decide to analyse them in terms of discrete statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Probability Distributions\n",
    "\n",
    "A **discrete probability distribution** describes how likely it is to find a given value for a random variable $X$ where $X$ is discrete, usually an integer.\n",
    "\n",
    "More formally, for a random variable $X$ with possible values in some **sample space** $\\Omega$, the corresponding probabilities $P(X)_\\Omega$ form a **probability distribution**. This is the distribution of probability for all possible values of the random variable $X$. \n",
    "\n",
    "Note that when dealing with discrete valued random variables $X$ as here, the probability distribution $P(X)$ is formally referred to as a \n",
    "[**probability mass function**](https://en.wikipedia.org/wiki/Probability_mass_function). This is the language used in many libraries such as `scipy.stats` used below where `pmf` is used to refer to the library function that returns $P(X)$ values, e.g. `scipy.stats.binom.pmf(k,n,p)` for the probability distribution of the binomial distribution. \n",
    "\n",
    "As the possible values $X$ are discrete, such as the result of rolling a die, we require that the sum of probabilities is one, i.e. $\\sum_{X \\in \\Omega} P(X) = 1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive example of a probability distribution can be seen by considering rolling a standard six-sided die  (one die, many dice). We can break this down as follows:\n",
    "\n",
    "* The sample space is $\\Omega$ = $\\{1, 2, 3, 4, 5, 6\\}$,\n",
    "* $P(X)$ = 1/6 for any possible outcome $X \\in \\Omega$ (assuming our die is unbiased).\n",
    "\n",
    "When we roll the die a single time, we are making a random draw from this probability distribution. As we roll the die more times and record the results, we should record a distribution of values whose frequency converges to a discrete uniform distribution .\n",
    "\n",
    "<!--  $U$ which will express as $X \\sim U(1, 6)$. A plot of this distribution can be seen in the figure below. SURELY THIS NOTATION IS FOR CONTINUOUS UNIFORM DISTRIBUTIONS \n",
    "In general, for a random variable $X$ distributed according to some uniform distribution $X \\sim U(a, b)$, we mean that X can take any values between $a$ and $b$ with equal probability. -->\n",
    "\n",
    "<img src='Week1_plots/one_die_distribution.png' align='center' width=600>\n",
    "\n",
    "*Probability distribution for the results of rolling one unbiased six-sided die. Each outcome is equally likely.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, probabilities do not have to be even in a probability distribution, and in general they are not. Consider rolling two standard, unbiased six-sided dice and adding the results. There is only one way to get the result of a 2 (rolling a 1 on both dice), but you can get the result of 3 in two different ways (rolling 1 and 2, or rolling 2 and 1). You can work this out for each possible value of the sum, and you find the distribution shown below.\n",
    "\n",
    "<img src='Week1_plots/two_dice_distribution_final.png' align='center' width=600>\n",
    "\n",
    "*Probability distribution for the results of rolling two unbiased six-sided dice and summing the results. The most likely outcome is 7.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "The uniform distribution is often important for Bayesian statistics, as if you have no physical model that informs your choice of prior distribution then assuming all values of the random variable are equally likely is typically the default. In this way, we can see this assumption as a statement of knowledge - if we know nothing about how a variable is distributed, it makes sense to assume all outcomes are equally likely until evidence proves otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example given here is a discrete probability distribution. We will talk about continuous probability distributions shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative distribution\n",
    "\n",
    "We can also define a **cumulative distribution**, which allows us to find the probability of our random variable being equal to or less than some value. We will from here onwards denote probability distributions and cumulative distributions using lower-case symbols and upper-case symbols respectively, say $f$ and $F$ respectively. \n",
    "\n",
    "Consider a random variable $X$ that takes values $\\{X_1, X_2, \\cdots, X_N\\}$. Let this random variable be distributed according to some probability distribution D with parameter $\\theta$, such that $X \\sim D(\\theta)$. We can write the probability of measuring a value $x$ of this random variable as\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x; \\theta) = P(X = x)\n",
    "\\end{equation*}\n",
    "\n",
    "The cumulative distribution $F_X(x; \\theta)$ tells us the probability that $X$ is less than or equal to $x$. For our distribution $f(X; \\theta)$, this is therefore given as:\n",
    "\n",
    "\\begin{equation*}\n",
    "F(x; \\theta) = P(X \\leq x) = \\sum_{X_i \\leq x}P(X = X_i) = \\sum_{X_i \\leq x}f(X_i; \\theta)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous probability distributions\n",
    "Of course we do not only encounter discrete random variables in experimental physics; indeed, many of the quantities measured e.g. in particle physics are continuous, such as energy or momentum. When considering values on a continuous scale, it does not make sense to consider defining a probability for a single value; instead, we must define a **probability density function**. For some random variable $X$ with probability density function $f(X)$, the *probability* of a value of $X$ between $a$ and $b$ is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "P(a \\leq X \\leq b) = \\int_a^b f(X) \\, dX\n",
    "\\end{equation*}\n",
    "\n",
    "As before, the probability density function must be normalised; this therefore requires that \n",
    "\n",
    "\\begin{equation*}\n",
    "\\int_\\Omega f(X)dX = 1,\\quad\\,\\, \\text{where}\\,\\, X \\in \\Omega.\n",
    "\\end{equation*}\n",
    "\n",
    "Note that whereas for a discrete random variable we summed over the probabilities the sample space $\\Omega$ (i.e. sum over each of the possible values of $X$), for a continuous random variable we must integrate the probability *density* over the sample space $\\Omega$. You can think of $f(X) \\, dX$ as an infinitesimal probability $dP$, the probability that we find an event between $X$ and $X+dX$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a cumulative distribution function $F(X)$ that as before defines the probability of measuring a value of $X$ less than or equal to some value $x$. (So this is a probability not a probability density).\n",
    "\n",
    "\\begin{equation*}\n",
    "F(X) = \\int_{X_a}^X f(X^\\prime) \\, dX^\\prime,\n",
    "\\end{equation*}\n",
    "\n",
    "where $X_a \\leq X \\leq X_b$ and $X_a$ and $X_b$ are the lower and upper bounds of the sample space $\\Omega$ respectively. This has particular use later when we discuss hypothesis testing.\n",
    "\n",
    "Consider a random variable $X$ distributed according to the following probability density function:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(X) = \\frac{3}{2\\cdot5^3}X^2, \\quad -5 \\leq X \\leq 5,\n",
    "\\end{equation*}\n",
    "where the factor $\\frac{3}{2\\cdot 5^3}$ is a normalisation factor such that $\\int_{-5}^5f(X) dX = 1$. This probability density function can be seen in the figure below.\n",
    "\n",
    "<img src='Week1_plots/continuous_pdf_example.png' align='center' width=600>\n",
    "\n",
    "*Continous probability density function $\\mathit{f(X) = X^2}$ for $\\mathit{-5 \\leq X \\leq 5}$. The distribution is normalised to integrate to 1 as required.* \n",
    "\n",
    "We can similarly define the CDF for this distribution, as\n",
    "\n",
    "\\begin{equation*}\n",
    "F(X) = \\int_{-5}^X f(X^\\prime)dX^\\prime = \\int_{-5}^X \\frac{3}{2\\cdot 5^3}{X^\\prime}^2 dX^\\prime = \\frac{X^3}{2\\cdot5^3} + \\frac{1}{2}, \\quad -5 \\leq X \\leq 5\n",
    "\\end{equation*}\n",
    "\n",
    "We can see this CDF behaves as expected; for the minimum value $X = -5$, $F(X = -5) = 0$, and for the maximum value $X = 5$, $F(X = 5) = 1$. This CDF is plotted in the figure below.\n",
    "\n",
    "<img src='Week1_plots/continuous_cdf_example.png' align='center' width = 600>\n",
    "\n",
    "*CDF of the previously shown probability distribution.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate probability distributions\n",
    "\n",
    "As well as probability distributions for single random variables, we can have probability distributions that describe more than one random variable. We can denote the **joint probability density function** for two random variables $X$ and $Y$ as $f(X,Y)$. If we want to find the probability density of one of these variables independent of the other, we want to calculate a **marginal density**. This is defined according to\n",
    "\n",
    "\\begin{equation*}\n",
    "g(X) = \\int_{Y_a}^{Y_b}f(X,Y)dY,\n",
    "\\end{equation*}\n",
    "where $g(X)$ is the marginal density for random variable $X$ and $Y_a \\leq Y \\leq Y_b$.\n",
    "\n",
    "For independent random variables $X$ and $Y$, the joint probability density function $f(X,Y)$ is just the product of the individual probability density functions $f(X)$ and $g(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "## Moments of probability distributions\n",
    "\n",
    "For both discrete and continuous probability distributions, we can use the distribution f(X) to obtain useful information about a particular random variable or a function of random variables. \n",
    "\n",
    "We can define the **expectation** of a random variable $X$ under a distribution $f(X)$ as\n",
    "\n",
    "\\begin{align*}\n",
    "E[X] &= \\sum_\\Omega Xf(X), \\quad \\text{for discrete f(X)} \\\\\n",
    "E[X] &= \\int_\\Omega Xf(X) dX, \\quad \\text{for continuous f(X)}\n",
    "\\end{align*}\n",
    "where in both cases $\\Omega$ denotes the space of possible values of $X$ under the distribution $f(X)$. \n",
    "\n",
    "This is also referred to as the **mean value** of a random variable $X$. The same formula can be applied to any function of a random variable; for a function $g(X)$, the expectation of this function under the distribution $f(X)$ is given as \n",
    "\n",
    "\\begin{equation*}\n",
    "E[g] = \\int_\\Omega g(X)f(X) dX\n",
    "\\end{equation*}\n",
    "\n",
    "The mean is often referred to as the **first moment** of the distribution of $X$; we can define higher moments as well. \n",
    "\n",
    "You will be familiar with the variance of data, which is defined as the expectation of the function $g(X) = (X - E[X])^2$.\n",
    "\n",
    "In general, we talk about two different kinds of moments: **algebraic moments** and **central moments**. These are defined as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_n &= E[X^n]\\quad\\text{is the }n^{th}\\text{ algebraic moment,}\\\\\n",
    "\\nu_n &= E[(X - E[X])^n]\\quad\\text{is the }n^{th}\\text{ central moment.}\n",
    "\\end{align*}\n",
    "\n",
    "From this we can see that the mean is the first algebraic moment $\\mu_1$, while the variance is the second central moment $\\nu_2$. \n",
    "\n",
    "You may have heard of other descriptors of probability distributions such as **skewness** and **kurtosis**, which are derived from the third and fourth moments respectively. \n",
    "\n",
    "We can also define moments for multi-dimensional distributions. For example, for a joint probability distribution $f(X,Y)$, the algebraic moment of order $m$ in $X$ and order $n$ in $Y$ can be written as $\\mu_{mn} = E[X^mY^n]$. \n",
    "\n",
    "The most commonly reelvant moment for multidimensional distributions is the **covariance**. \n",
    "\n",
    "* Consider a collection of $M$ random variables $\\mathbf{X} = \\{X_1,\\cdots X_M\\}$\n",
    "* The covariance between variables $X_i$ and $X_j$ is the central moment of order 1 in $X_i$ and $X_j$ and order 0 for the remaining $M - 2$ variables:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{covariance}(X_i,X_j) = \\nu^{ij}_{1,1} = E[(X_i - E[X_i])(X_j-E[X_j])]\n",
    "\\end{equation*}\n",
    "\n",
    "The covariance gives us an idea of how dependent on one another two random variables are, specifically how much variance in one variable affects the other variable. We can then define the correlation between the random variables $X_i$ and $X_j$ as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{correlation}(X_i,X_j) = \\frac{\\nu^{ij}_{1.1}}{\\sqrt{\\nu_2^i\\nu_2^j}},\n",
    "\\end{equation*}\n",
    "where $\\nu_2^i$ and $\\nu_2^j$ are the variance of $X_i$ and $X_j$ respectively. The value of the correlation coefficient will be between -1 and 1; a value of 1 indicates the two variables are perfectly correlated, a value of -1 indicates they are perfectly anti-correlated, whereas a value of 0 indicates the two variables are **uncorrelated**.\n",
    "\n",
    "If two random variables are independent, then the corresponding correlation coefficient is equal to 0 and the variables are uncorrelated. However, while in general independence implies the variables are uncorrelated, uncorrelated variables are not necessarily independent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next sections will cover three of the most common probability distributions. All of these are covered in the PHYS40005 Statistics Of Measurement course so check your notes from that course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen the Binomial distribution before; this is a discrete probability distribution for modelling random variables with a binary outcome, i.e. one of two possible values. We can use this distribution to predict the number of successes $k$ in a number of independent trials (attempts) $n$, where the probability of success for any one trial is $p$. We can write this distribution as:\n",
    "\\begin{equation*}\n",
    "f(k; n, p) = {n \\choose k}p^k q^{n-k},\n",
    "\\end{equation*}\n",
    "where:\n",
    "* $k$ is the number of successes\n",
    "* $n$ is the number of trials\n",
    "* $p$ is the probability of success\n",
    "* $q$ = 1 - $p$ is the probability of failure\n",
    "* ${n \\choose k} = \\frac{n!}{k!(n - k)!}$, known as the \"binomial coefficient\"\n",
    "\n",
    "The Binomial distribution is sometimes denoted as $B(n,p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that the expectation and variance of a binomially distributed variable with $n$ trials and probability of success $p$ are equal to $np$ and $np(1-p)$ respectively, although the proof is not shown in this course.\n",
    "\\begin{equation}\n",
    "\\langle k \\rangle = \\sum_{k=0}^n k f(k;n,p) = n\\, p\n",
    "\\, , \\quad\n",
    "\\langle k^2 \\rangle = \\sum_{k=0}^n k^2 f(k;n,p) = n\\, p \\, (1-p) \\, .\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative distribution $F(k; n, p)$ gives the probability of finding $k$ or fewer successes, and it is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "F(k; n, p) = \\sum_{i = 0}^k f(k;n,p) = \\sum_{i = 0}^k {n \\choose k}p^k q^{n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "This is not a particularly simple function (the incomplete normalised Beta function if you must know). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is implemented in `scipy.stats`. It is easy to plot the probability distribution `scipy.stats.binom.pmf(k,n,p)` and the cumulative distribution `scipy.stats.binom.cdf(k,n,p)` for a range of values of $k$ for given $n$ and $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "k = np.arange(0,40)\n",
    "\n",
    "# binom.pmf(k, n, p) for n trials, k positive results with probability p for a single positive result from one pick \n",
    "# List of values and their colours to use in plots\n",
    "n_list = [25,   25,  50, 50] \n",
    "p_list = [0.1, 0.4, 0.1, 0.4]\n",
    "colors = ['black','#D55E00','#56B4E9','#E69F00']\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize = (14,6))\n",
    "\n",
    "# Now plot 4 different sets of values on each of two subplots accessed via ax[0] and ax[1].\n",
    "for i in range(4):  \n",
    "    axs[0].scatter(k,binom.pmf(k,n_list[i],p_list[i]),label='$B({:.0f},{:.2f})$'.format(n_list[i],p_list[i]),color=colors[i])\n",
    "    # Now we could sum up the pdf (pmf) by hand to get the cdf (cumulative distribution) but we are lazy and just use the bnuilt in cdf\n",
    "    axs[1].scatter(k,binom.cdf(k,n_list[i],p_list[i]),label='$B({:.0f},{:.2f})$'.format(n_list[i],p_list[i]),color=colors[i]) \n",
    "\n",
    "axs[0].set_xlabel('$k$',fontsize = 16)\n",
    "axs[0].set_ylabel('$f(k;n,p)$',fontsize = 16)\n",
    "axs[0].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[0].xaxis.set_minor_locator(MultipleLocator(1))\n",
    "axs[0].yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "axs[0].set_title('Probability distribution',fontsize = 20)\n",
    "\n",
    "axs[1].set_xlabel('$k$',fontsize = 16)\n",
    "axs[1].set_ylabel('$F(k;n,p)$',fontsize = 16)\n",
    "axs[1].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[1].xaxis.set_minor_locator(MultipleLocator(1))\n",
    "axs[1].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[1].set_title('Cumulative distribution',fontsize = 20)\n",
    "\n",
    "\n",
    "axs[0].legend(loc='upper right',fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Poisson distribution is a discrete probability distribution that is used to model counts. As the possible events here are different count values, these are non-negative integers but without any limit. Compare to the Binomial distribution where the count was limited by $n$, the number of trials. \n",
    "\n",
    "The Poisson distribution is appropriate for problems where we are counting events in a fixed period of time (or space) that occur randomly and independently but each at the same mean rate (or mean density). \n",
    "\n",
    "<!-- This distribution is valid for:\n",
    "* Events that occur in a fixed interval of time or space\n",
    "* Events are independent of the time since the previous event -->\n",
    "\n",
    "Probability for a given number of events $k$, with a mean rate $\\lambda$, is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "f(k; \\lambda) = \\frac{\\lambda^ke^{-\\lambda}}{k!}\n",
    "\\end{equation*}\n",
    "\n",
    "As before the CDF can be calculated by summing the probability distribution up to the evaluation value. \n",
    "\n",
    "The expectation and variance can be shown to both be equal to $\\lambda$\n",
    "\\begin{equation}\n",
    "\\langle k \\rangle = \\sum_{k=0}^\\infty k f(k;\\lambda) \n",
    "\\quad = \\quad\n",
    "\\langle k^2 \\rangle = \\sum_{k=0}^n k^2 f(k;\\lambda) \\quad = \\lambda \\, .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Poisson distribution is implemented in `scipy.stats`, so it is easy to visualise the probability as a function of $k$ for a given $\\lambda$ along with the CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "# In principle, k is any non-negative integer but we will only go upto 20 in this example. \n",
    "k = np.arange(0,20)\n",
    "\n",
    "# List of four lambda values to use in Poisson distribution examples\n",
    "lambdas = np.arange(0.1,16,4.0)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize = (14,6))\n",
    "\n",
    "# Now compare the use of a list we never use to set the plot values as opposed to a simple loop used for Binomial.\n",
    "# This form using a \"list comprehension\" is faster but for such a small set of values we will never notice the difference.\n",
    "# I would normally use the simpler loop here as I don't need speed here but clarity is always valuable.\n",
    "# However, to illustrate this trick I will use the list comprehension here.\n",
    "[axs[0].scatter(k,poisson.pmf(k,lambdas[i]),label='$P({:.1f})$'.format(lambdas[i]),color=colors[i]) for i in range(4)]\n",
    "[axs[1].scatter(k,poisson.cdf(k,lambdas[i]),label='$P({:.1f})$'.format(lambdas[i]),color=colors[i]) for i in range(4)]\n",
    "\n",
    "axs[0].set_xlabel('$k$',fontsize = 16)\n",
    "axs[0].set_ylabel('$f(k;\\lambda)$',fontsize = 16)\n",
    "axs[0].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[0].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[0].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[0].set_title('Probability distribution',fontsize = 20)\n",
    "\n",
    "axs[1].set_xlabel('$k$',fontsize = 16)\n",
    "axs[1].set_ylabel('$F(k;\\lambda)$',fontsize = 16)\n",
    "axs[1].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[1].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[1].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[1].set_title('Cumulative distribution',fontsize = 20)\n",
    "\n",
    "\n",
    "axs[0].legend(loc='upper right',fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution as a limit of the Binomial distribution\n",
    "\n",
    "Consider a binomial distribution $f(k; n, p)$. Think of the limit where we have a large number of samples $n$ but a very small probability of success $p$ such that the product $\\lambda = np$ is constant. In this case the binomial distribution in this limit converges to the Poisson distribution $f(k; \\lambda = n \\, p)$\n",
    "\\begin{equation*}\n",
    "\\lim_{n\\to\\infty,\\,p\\to0}f(k; n, p) = \\frac{(np)^k}{k!}e^{-np} = \\frac{\\lambda^k}{k!}e^{-\\lambda} = f(k; \\lambda),\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "#### Optional Example: radioactive atoms.\n",
    "    \n",
    "Suppose the probability any one atom decays in an interval $t$ as measured in our Geiger counter. This is a binary result, the atom decays or its doesn't.  So if we have $N$ atoms, each can decay in time $t$ with the same probability $p$ as every other atom (i.e. events are statistically independent).   The probability of measuring $k$ decays in time $t$ is given by a Binomial distribution. We would get a good fit by measuring the average number of deacys $\\langle k \\rangle$ in time $t$ and then setting $\\langle k \\rangle = p \\, N $.  \n",
    "    \n",
    "However, our same in the lab probably has a few milligrammes of radioactive material so is likely to have around $N \\sim 10^{20}$ radiactive atoms which is essentially an infinite number of trials $n=N$ in this context. So in practice, we will not be able to tell the difference between a Binomial distribution and a Poisson distribution with $\\lambda = \\langle k \\rangle = p \\,N$ when trying to find which gives the best fit to the data from many measurements of the counts in time intervals of length $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "### Poisson distribution as a limit of the Binomial distribution\n",
    "\n",
    "Consider a binomial distribution with a large number of samples, but a very small probability of success. We can consider this limit as $n \\to \\infty$ and $p \\to 0$. Assuming that these limits are taken such that the product $\\lambda = np$ is constant, we can show that the binomial distribution in this limit converges to the Poisson distribution.\n",
    "\n",
    "We can rewrite the binomial distribution using $p = \\frac{\\lambda}{n}$:\n",
    "\n",
    "\\begin{align*}\n",
    "f(k; n, p) &= \\frac{n!}{k!(n-k)!}\\left(\\frac{\\lambda}{n}\\right)^k\\left(1 - \\frac{\\lambda}{n}\\right)^{n-k} \\\\\n",
    "&= \\frac{\\lambda^k}{k!}\\frac{n!}{(n-k)!}\\left(\\frac{1}{n}\\right)^k\\left(1 - \\frac{\\lambda}{n}\\right)^{n-k}\n",
    "\\end{align*}\n",
    "\n",
    "We can look at a couple terms of this expression separately to determine how this behaves in the limit $n \\to \\infty$, starting with the middle term.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{n!}{(n-k_!)}\\left(\\frac{1}{n}\\right)^k = \\frac{n(n-1)(n-2)\\cdots(n - k + 1)(n -k)(n-k - 1)\\cdots}{n^k(n-k)(n-k-1)\\cdots}\n",
    "\\end{equation*}\n",
    "\n",
    "Terms from (n-k) and downwards to 1 can be cancelled between the numerator and denominator, resulting in\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{n!}{(n-k_!)}\\left(\\frac{1}{n}\\right)^k = \\frac{n}{n^k}(n-1)(n-2)\\cdots(n-k+1)\n",
    "\\end{equation*}\n",
    "\n",
    "We can take a factor of n out of each of the $k - 1$ bracketed terms on the right hand side, leading to\n",
    "\\begin{equation*}\n",
    "\\frac{n!}{(n-k_!)}\\left(\\frac{1}{n}\\right)^k = \\frac{n^k}{n^k}\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)\\cdots\\left(1-\\frac{k+1}{n}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "In the limit $n \\to \\infty$, each time like $\\frac{1}{n}$, $\\frac{2}{n}$ etc will tend to 0, thus this whole term tends to 1 as we take the limit $n \\to \\infty$ and $p \\to 0$.\n",
    "\n",
    "Now considering the last term in our original expression:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left(1 - \\frac{\\lambda}{n}\\right)^{n-k} = \\left(1 - \\frac{\\lambda}{n}\\right)^n\\left(1-\\frac{\\lambda}{n}\\right)^{-k}\n",
    "\\end{equation*}\n",
    "\n",
    "As $n\\to\\infty$ and $p\\to 0$, $\\lambda$ is constant. As a result, the second term tends to 1. The first term is more interesting, and in fact looks a lot like the usual expression for the exponential function,\n",
    "\n",
    "\\begin{equation*}\n",
    "e^x = \\lim_{n\\to\\infty}\\left(1 + \\frac{x}{n}\\right)^n,\n",
    "\\end{equation*}\n",
    "\n",
    "so we find that:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\lim_{n\\to\\infty,\\,p\\to0}\\left(1 - \\frac{\\lambda}{n}\\right)^{n-k} = e^{-\\lambda} = e^{-np}\n",
    "\\end{equation*}\n",
    "\n",
    "We can therefore put everything together to show that\n",
    "\n",
    "\\begin{equation*}\n",
    "\\lim_{n\\to\\infty,\\,p\\to0}f(k; n, p) = \\frac{(np)^k}{k!}e^{-np} = \\frac{\\lambda^k}{k!}e^{-\\lambda} = f(k; \\lambda),\n",
    "\\end{equation*}\n",
    "where this is the Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal/Gaussian distribution\n",
    "\n",
    "The **normal distribution**, also known as the **Gaussian distribution**, is the third and last distribution we shall review as this is often seen and is extremely useful. \n",
    "\n",
    "The reason that the normal distribution appears so often is explained by a rigourous mathematical result known as the **central limit theorem** which states that for any ensemble of $N$ random variables, the distribution of the total, the sum of the random variables, tends to a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "Aside: As far as I can see which name you use if a matter of personal preference. I am guessing that different groups of researchers prefer one name over the other, perhaps statisticians use the term *normal distribution* while I usually call it the *Gaussian distribution*.  You will see that statistical modules are usually written by/for statisticians so the routines we will use for this distribution come under the name `scipy.stats.norm` (\"norm\" for \"normal\", not sure why they had to drop just two characters!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution is a **continuous probability distribution**. It is a family of functions described by two parameters: the mean $\\mu$ and the standard deviation $\\sigma$. So the normal distribution gives the possible results for a **continuous** random variable $X$, which we may write formally as  $X \\sim N(\\mu,\\sigma)$. The probability density function is given as\n",
    "\n",
    "\\begin{equation*}\n",
    "f(X; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{X - \\mu}{\\sigma}\\right)^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again `scipy.stats` defines the PDF and CDF for a normal distribution, that you can access easily:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x = np.arange(-10,10,0.1)\n",
    "mu_range = [-2,0,5]\n",
    "sig_range = [0.5,2,1]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize = (14,6))\n",
    "[axs[0].plot(x, norm.pdf(x,m,s),label = 'N({:.1f},{:.1f})'.format(m,s), color = c) for m, s, c in zip(mu_range,sig_range,colors)]\n",
    "[axs[1].plot(x, norm.cdf(x,m,s),label = '$\\mu$ = {:.1f}, $\\sigma$ = {:.1f}'.format(m,s), color = c) for m, s, c in zip(mu_range,sig_range, colors)]\n",
    "\n",
    "axs[0].set_xlabel('$X$',fontsize = 16)\n",
    "axs[0].set_ylabel('$f(X; \\mu, \\sigma)$',fontsize = 16)\n",
    "axs[0].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[0].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[0].yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "axs[0].set_title('Probability distribution',fontsize = 20)\n",
    "axs[0].set_ylim(bottom = -0.02)\n",
    "\n",
    "axs[1].set_xlabel('$X$',fontsize = 16)\n",
    "axs[1].set_ylabel('$F(X;\\mu,\\sigma)$',fontsize = 16)\n",
    "axs[1].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[1].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[1].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[1].set_title('Cumulative distribution',fontsize = 20)\n",
    "axs[1].set_ylim(bottom = -0.02)\n",
    "\n",
    "axs[0].legend(loc='upper right', fontsize = 12)\n",
    "fig.suptitle('Gaussian random variable',fontsize = 26)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal distribution with $\\mu$ = 0 and $\\sigma$ = 1 is referred to as a **standard normal distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "## Central limit theorem\n",
    "\n",
    "You will have heard of the central limit theorem before, but we can and will prove it here. This theorem states that for a set of independent and identically distributed random variables (i.e. different samples of the same probability distribution), the distribution of the standardised sample mean tends towards the standard normal distribution, regardless of the distribution of the original variables. \n",
    "\n",
    "In practice, this means if we make many measurements of the same quantity, the mean and variance of those measurements will tend towards a normal distribution as we increase the number of measurements. \n",
    "\n",
    "To prove this, we will require **characteristic functions**. Any probability distribution has a characteristic function that completely defines it - if we know $\\varphi_X(t)$, we know $f(X)$ too. This function is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_X(t) = \\int_{-\\infty}^\\infty e^{itX}f(X)dX\n",
    "\\end{equation*}\n",
    "\n",
    "This is equivalent to taking the Fourier transform of the probability density function. All properties of Fourier transforms can therefore be applied to characteristic functions. In particular, the one we will make use of for proving the central limit theorem is the sum-product relation: the characteristic function of the sum of independent variables is the product of the individual characteristic functions, i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{X_1 + \\cdots + X_n}(t) = \\prod_{i = 1}^n\\varphi_{X_i}(t)\n",
    "\\end{equation*}\n",
    "\n",
    "### Central limit theorem for independent and identically distributed random variables\n",
    "\n",
    "\n",
    "* Consider a set of $n$ independent random variables $X_i$, each with mean $\\mu$ and variance $\\sigma^2$\n",
    "* The sum of these random variables $X_1 + \\cdots + X_n$ has a mean $n\\mu$ and variance $n\\sigma^2$ (as variables are independent)\n",
    "* Define the standardised sum of these random variables:\n",
    "\n",
    "\\begin{equation*}\n",
    "Z_n = \\frac{X_1 + \\cdots +X_n - n\\mu}{\\sqrt{n\\sigma^2}} = \\sum_{i = 1}^n\\frac{X_i - \\mu}{\\sqrt{n\\sigma^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Define standardised random variables $Y_i = \\frac{X_i - \\mu}{\\sigma}$ with mean = 0 and variance = 1, such that\n",
    "\n",
    "\\begin{equation*}\n",
    "Z_n = \\sum_{i = 1}^n\\frac{1}{\\sqrt{n}}Y_i\n",
    "\\end{equation*}\n",
    "\n",
    "* Using the definition of the characteristic function, the characteristic function $\\varphi_{Z_n}$ is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{Z_n}(t) = \\varphi_{Y_1}\\left(\\frac{t}{\\sqrt{n}}\\right)\\varphi_{Y_2}\\left(\\frac{t}{\\sqrt{n}}\\right)\\cdots\\varphi_{Y_n}\\left(\\frac{t}{\\sqrt{n}}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "* Because the Y_i are identically distributed, this is equal to\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{Z_n}(t) = \\left[\\varphi_{Y_1}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^n\n",
    "\\end{equation*}\n",
    "\n",
    "* As $n \\to \\infty$, the argument of $\\varphi_{Y_1}$ becomes small, so we can apply a Taylor expansion:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{Y_1}\\left(\\frac{t}{\\sqrt{n}}\\right) = E\\left[e^{i\\frac{t}{\\sqrt{n}}Y_i}\\right] = E\\left[\\sum_{r = 0}^\\infty\\frac{\\left(i\\frac{t}{\\sqrt{n}}Y_1\\right)^r}{r!}\\right] = \\sum_{r = 0}^\\infty\\frac{\\left(i\\frac{t}{\\sqrt{n}}\\right)^r}{r!}E[Y_i^r]\n",
    "\\end{equation*}\n",
    "\n",
    "Because we have standardised $Y_i$, $E[Y_1]$ = 0 and the variance $V(Y_1) = E[Y^2] - E[Y]^2 = E[Y^2]$ = 1, the first three terms of the Taylor expansion evaluate to give\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{y_1}\\left(\\frac{t}{\\sqrt{N}}\\right) = 1 - \\frac{t^2}{2n} + \\mathcal{O}\\left(\\frac{t^3}{N^\\frac{3}{2}}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "We can substitute this back into the expression for $\\varphi_{Z_n}$ to find that\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{Z_n}(t) = \\left(1 - \\frac{t^2}{2n}\\right)^n\n",
    "\\end{equation*}\n",
    "\n",
    "Again from our standard form of the exponential, in the limit of $n\\to\\infty$ we can see that\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varphi_{Z_n}(t) = e^{-\\frac{1}{2}t},\n",
    "\\end{equation*}\n",
    "which is the characteristic function for a normal distribution with $\\mu$ = 0 and $\\sigma$ = 1. As a result, by a theorem known as Lévy's convergence theorem, we can say that in the limit of $n\\to\\infty$ the random variable $Z_n$ is distributed according to a standard normal distribution, $Z\\sim N(0,1)$. This is regardless of the specifics of the distribution the random variables $X_i$ are distributed according to.\n",
    "\n",
    "### Practical implications\n",
    "\n",
    "For the binomial and Poisson distributions, it can be shown that the sum of outcomes of a binomial is distributed according to another binomial distribution, and the sum of Poissons is distributed according to another Poisson distribution. \n",
    "\n",
    "To demonstrate this, consider a binomially distributed variable. Taking $N$ trials, then taking another $N$ trials, is the same as just taking $2N$ trials to begin with. For a Poisson distribution, consider Poisson random variable with expectation $\\lambda$. If $N$ measurements are made, the total average number expected over these $N$ trials is $N\\lambda$. This is equivalent to making a single measurement of a Poisson random variable with expectation $N\\lambda$. \n",
    "\n",
    "At what scale is it appropriate to approximate a binomial or Poisson distribution with a normal distribution? It depends on the parameters of the distributions; for binomial distributions, we need to make the number of trials $n$ sufficiently large, whereas for the Poisson distribution we need sufficiently large $\\mu$. Typically, a value of $\\mu$ around 100 gives a reasonable Gaussian approximation to the Poisson distribution, whereas for a binomial the necessary value of $n$ depends on the probability $p$ and generally requires $1\\ll Np \\ll N$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see next week how this distribution is used when trying to find out how well some data is represented by a function found by fitting to that data, something known as goodness of fit test and specifically the $\\chi^2$ test for goodness of fit.\n",
    "\n",
    "The $\\chi^2$ distribution (also known as the \"chi-square\" or incorrectly but commonly as the \"chi-squared\" so use whatever you like) is a very important distribution that is related to the normal distribution. It is a family of continuous probability distribution with one parameter $N_\\mathrm{dof}$, the number of degrees of freedom. The $\\chi^2(N_\\mathrm{dof})$ distribution describes the likelihood of finding a value for a continuous random variable $X$ which is equal to the sum of the squares of $N_\\mathrm{dof}$ independent random variables drawn from a standard normal distribution (mean $0$, standard deviation $1$). Note that $X$ is continuous but here cannot be negative.\n",
    "\n",
    "That is if we have $N_\\mathrm{dof}$ independent, standard normal variables $\\{ Z_1$, $\\cdots$, $Z_N \\}$, we can look at the distribution of the sum of their squares, $Q$, \n",
    "\\begin{equation}\n",
    "Q = \\sum_{i = 1}^{N_\\mathrm{dof}} Z_i^2 \\, .\n",
    "\\end{equation}\n",
    "Then $Q$ is distributed according to a $\\chi^2$ distribution with $N_\\mathrm{dof}$ degrees of freedom, which we write formally as $Q \\sim \\chi^2(N)$\n",
    "\n",
    "Once again `scipy.stats` has a good implementation of the $\\chi^2$ distribution, for any given number of degrees of freedom, `scipy.stats.chi2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "x = np.arange(0.1,20,0.05)\n",
    "ndof = range(1,12,3)\n",
    "\n",
    "# plotting\n",
    "fig, axs = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "[ axs[0].plot(x,chi2.pdf(x,ndof[i]),label=\"$\\chi^{2}(%d)$\"%ndof[i],color=colors[i]) for i in range(4) ]\n",
    "[ axs[1].plot(x,chi2.cdf(x,ndof[i]),label=\"$\\chi^{2}(%d)$\"%ndof[i],color=colors[i]) for i in range(4) ]\n",
    "\n",
    "axs[0].set_xlabel('$X$',fontsize = 16)\n",
    "axs[0].set_ylabel('$f(X; n)$',fontsize = 16)\n",
    "axs[0].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[0].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[0].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[0].set_title('Probability distribution',fontsize = 20)\n",
    "axs[0].set_ylim(bottom = -0.02)\n",
    "\n",
    "axs[1].set_xlabel('$X$',fontsize = 16)\n",
    "axs[1].set_ylabel('$F(X;n)$',fontsize = 16)\n",
    "axs[1].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[1].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[1].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[1].set_title('Cumulative distribution',fontsize = 20)\n",
    "axs[1].set_ylim(bottom = -0.02)\n",
    "\n",
    "axs[0].legend(fontsize = 12)\n",
    "fig.suptitle('$\\chi^2$ random variable',fontsize = 26)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one key property you need to know and that is the mean of this distribution is roughly equal to the number of degrees of freedom $N_\\mathrm{dof}$. You can see this since a typical random number $X$ drawn from a standard normal distribution is going to lie between $-1$ and $1$ about 68% of the time and between $-2$ and $2$ about 95% of the time. So a good bet is that the average value of $X^2$ is roughly $1$. Given the  $N_\\mathrm{dof}$ values of $X^\"$ are independent but all from the same distribution, that means a fair guess is that the mean is about equal to the number of degrees of freedom.\n",
    "\n",
    "In fact, this can be done exactly and it turns out to be exactly true, that the mean is always equal to the number of degrees of freedom\n",
    "$$\n",
    "\\langle Q \\rangle = \\int_0^\\infty Q \\, f_\\chi(Q; N_\\mathrm{dof}) \\, dQ = N_\\mathrm{dof} \\, .\n",
    "$$\n",
    "\n",
    "Below we will repeat the plots of the $\\chi^2$ distribution but rescaling the horizontal axis by the number of degrees of freedom. You will see the curves for different $N_\\mathrm{dof}$ are much more similar now, especially for larger $N_\\mathrm{dof}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "# xr is X/N_{dof}\n",
    "xr = np.arange(0.1,2.5,0.005)\n",
    "ndof = range(1,12,3)\n",
    "\n",
    "# plotting\n",
    "fig, axs = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "for i in range(4):\n",
    "    axs[0].plot(xr,chi2.pdf(xr*ndof[i],ndof[i]),label=\"$\\chi^{2}(%d)$\"%ndof[i],color=colors[i]) \n",
    "    axs[1].plot(xr,chi2.cdf(xr*ndof[i],ndof[i]),label=\"$\\chi^{2}(%d)$\"%ndof[i],color=colors[i]) \n",
    "\n",
    "axs[0].set_xlabel('$X/N_\\mathrm{dof}$',fontsize = 16)\n",
    "axs[0].set_ylabel('$f(X; N_\\mathrm{dof})$',fontsize = 16)\n",
    "axs[0].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[0].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[0].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[0].set_title('Probability distribution',fontsize = 20)\n",
    "axs[0].set_ylim(bottom = -0.02)\n",
    "\n",
    "axs[1].set_xlabel('$X/N_\\mathrm{dof}$',fontsize = 16)\n",
    "axs[1].set_ylabel('$F(X;N_\\mathrm{dof})$',fontsize = 16)\n",
    "axs[1].tick_params(labelsize = 12,direction='in',top=True,right=True,which='both')\n",
    "axs[1].xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axs[1].yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "axs[1].set_title('Cumulative distribution',fontsize = 20)\n",
    "axs[1].set_ylim(bottom = -0.02)\n",
    "\n",
    "axs[0].legend(fontsize = 12)\n",
    "fig.suptitle('$\\chi^2$ random variable',fontsize = 26)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have discussed probability distributions, including:\n",
    "\n",
    "* Definitions of probability distributions and cumulative distributions\n",
    "* Differences between discrete and continuous probability distributions\n",
    "* Specific probability distributions, including binomial, poisson, normal and $\\chi^2$, and their implementations in `scipy`\n",
    "\n",
    "In the next section, we will discuss how we can generate data from a given probability distribution using Monte Carlo methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "### Non-examinable mathematics: The $\\chi^2$ distribution for one degree of freedom, $N_\\mathrm{dof}=1$    \n",
    "    \n",
    "Finding the $\\chi^2$ distribution for one degree of freedom, $N_\\mathrm{dof}=1$ is simple but illustrates has a couple of key points that often turn up (and so when missed, lead to errors) when making a change of change of variables with continuous probability distributions.\n",
    "    \n",
    "Consider the distribution of $Q$, the square of a single random variable $X$ which is drawn from a standard normal dtstribution, $N(0,1)$, so mean $0$ and standard deviation $1$.  Thus random variable $Q=X^2$ is distributed according to the $\\chi^2$ distribution with one degree of freedom, $N_\\mathrm{dof}=1$. \n",
    "\n",
    "We want to find the probability $f_\\chi(Q;1)dQ$ of finding a result between $Q$ and $Q+dQ$ where $Q=X^2$. There are two events in $X$ space that can contribute to this value of for the probability density at value $Q$: events at values $X=+\\sqrt{Q}$ and $X=-\\sqrt{Q}$ both contribute.    \n",
    "We know that the probability of finding a results between $X$ and $X+dX$ must be\n",
    "\\begin{equation}\n",
    "  f_N(X)dX = \\frac{1}{\\sqrt{2\\pi}} \\exp(-X^2/2)    \n",
    "\\end{equation}\n",
    "and between $-X$ and $-X+dX$ is\n",
    "\\begin{equation}\n",
    "  f_N(-X)dX = \\frac{1}{\\sqrt{2\\pi}} \\exp(-(-X)^2/2)    = f_N(X)dX \\, .\n",
    "\\end{equation}\n",
    "\n",
    "Changing the variable we use to describe the event cannot change the probability of the actual event occuring.  So we must have that  \n",
    "\\begin{equation}\n",
    "  f_\\chi(Q;1)dQ = f_N(X)dX + f_N(-X)dX = \\frac{2}{\\sqrt{2\\pi}} \\exp(-X^2/2)\\, dX\n",
    "\\end{equation}\n",
    "\n",
    "The other key trick here is that we must remember that the size of the infinitesimal changes when we change variables and this is implicit in the definition of a probability density. That is $dQ \\neq dX$ in general. However since $Q=X^2$ we can differentiate to see that $dQ/dX =2 X = 2 \\sqrt{Q}$ so then we have that \n",
    "$$\n",
    "\\begin{align}\n",
    "  f_\\chi(Q;1)dQ \n",
    "    & = \\frac{2}{\\sqrt{2\\pi}} \\exp(-Q/2)  \\frac{1}{dQ/dX}  dQ \\\\\n",
    "    & = \\frac{2}{\\sqrt{2\\pi}} \\exp(-Q/2)  \\frac{1}{2 \\sqrt{Q}}  dQ \\\\\n",
    "    & = \\frac{1}{\\sqrt{2\\pi Q}} \\exp(-Q/2)  dQ \\, .\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore we see that the probability density (probability mass function) for a $\\chi^2$ distribution with one degree of freedom is\n",
    "$$\n",
    "  f_\\chi(Q;1) = \\frac{1}{\\sqrt{2\\pi Q}} \\exp(-Q/2) \\, .\n",
    "$$\n",
    "In fact, there is a closed form for the [$\\chi^2$ distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution) for any $N_\\mathrm{dof}$.\n",
    "    \n",
    "We don't need these exact formulae here. The detailed numerical evaluations are always available in efficient library functions. All we need is the intuitive results given above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Five: Monte Carlo methods of data generation [^](#outline)<a id='section-5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will need to simulate data distributions from experiments or theories. For instance this helps us prepare in advance for the time when we take data from a large experiment. It is also useful when we want to compare real data with different theoretical predictions. This is called the **Monte Carlo Technique** and is in effect a form of integration. This is often used for functions that are difficult to integrate or invert. However, we will begin with the method for simple situations where we have analytical forms for all the functions we need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse transform \n",
    "\n",
    "The mathematics given below shows that fundamental ideas behind generating the required random distribution from a known probability distribution function $f(X)$ . \n",
    "\n",
    "The aim is to generate a random number $X$ from the pdf (probability distribution function) $f(X)$ in the range $-\\infty \\le X \\le \\infty$. To do this we actually work with the CDF (cumulative distribution function) and we follow these steps:\n",
    "1. Calculate the cumulative distribution $F(X) = \\int_{-\\infty}^X f(X') \\, dX'$.\n",
    "1. For properly normalised PDF (and CDF), generate a random number $u$ drawn from the uniform distribution $U(0,1)$ i.e. between $0$ and $1$.\n",
    "1. Find $X$ such that $F(X) = u$ so that $X=F^{-1}(u)$. (Note that this is effectively a definition of the the \"inverse function\" $F^{-1}$ which we look at below.)\n",
    "\n",
    "It is straightforward to repeat this as many times as you need to get a sinmulated data set drawn from the specified distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of a function\n",
    "\n",
    "In order to complete step 3, we must find the **inverse function** of the CDF, which inverts the operation of the CDF. We can write this as\n",
    "\\begin{align*}\n",
    "F(X) &= u\\\\\n",
    "F^{-1}(u) &= X\n",
    "\\end{align*}\n",
    "Here the notation $F^{-1}$ indicates we are defining the inverse function of the function $F$. \n",
    "\n",
    "The inverse function $F^{-1}(u)$ of any function $F$ simply reverses the effect of the original function so \n",
    "$$\n",
    "F^{-1}( F(X) ) = F^{-1}( u )= X \n",
    "$$\n",
    "where $u = F(X)$.\n",
    "\n",
    "Another way to look at the inverse function is it is the \"reflection\" of a plot of $F(x)$ vs. $X$ where we swap the usual axes.  That is if we plot values where for each point the horizontal axis coordinate is $x=F(X)$ and the vertical axis correpsonding horizontal coordinate is $y=X$. We will use this trick later where once we have the points we use numerical methods to define a smooth curve between the data points, the process of [interpolation](https://en.wikipedia.org/wiki/Interpolation).\n",
    "\n",
    "For simple functions, the inverse is obvious. For instance, if $F(x) = X+2$ then $F^{-1}(u) = u -2$ as $F^{-1}( F(X) ) = F^{-1}( X+2)$. If we now let $u=X+2$ then $F^{-1}( F(X) ) = F^{-1}( u) = u-2 = X$ as required.\n",
    "\n",
    "For complicated functions, the inverse maybe impossible to find analytically so this may have to be done numerically. \n",
    "<!-- One way is to solve the equation $F(X) - u=0$ to find $X$ where function $F$ and value $u$ are known. -->\n",
    "\n",
    "So this method is exact only for probability distribution functions which are integrable and invertable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete step 3, we must find the **inverse function** of the CDF, which inverts the operation of the CDF. We can write this as\n",
    "\\begin{align*}\n",
    "F(X) &= u\\\\\n",
    "F^{-1}(u) &= X\n",
    "\\end{align*}\n",
    "where $F^{-1}(u)$ is the inverse function of the CDF. The inverse function $F^{-1}(u)$ of any function $F$ simply reverses the effect of the original function so $F^{-1}( F(X) ) = F^{-1}( u )= X$.  \n",
    "\n",
    "For simple functions, the inverse is obvious. For instance, if $F(x) = X+2$ then $F^{-1}(u) = u -2$ as $F^{-1}( F(X) ) = F^{-1}( X+2)$. If we now let $u=X+2$ then $F^{-1}( F(X) ) = F^{-1}( u) = u-2 = X$ as required.\n",
    "\n",
    "For complicated functions, the inverse maybe impossible to find analytically so this may have to be done numerically. That is you have to solve the equation $F(X) - u=0$ to find $X$ where function $F$ and value $u$ are known.\n",
    "\n",
    "So this method is exact only for probability distribution functions which are integrable and invertable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the inverse transform method $f(X) \\propto X^2$\n",
    "\n",
    "To illustrate this, we will go through a concrete example using the PDF given as an example earlier: $f(X) \\propto X^2$ for $-5\\leq X \\leq 5$. The PDF and CDF are given as\n",
    "\n",
    "\\begin{align*}\n",
    "            & f(X) = \\frac{3}{2\\cdot5^3}X^2, \\quad &-5 \\leq X \\leq 5 \\\\\n",
    "\\Rightarrow & F(X) = \\int_{-5}^X f(X) \\, dX = \\frac{X^3}{2\\cdot5^3} + \\frac{1}{2}, \\quad &-5 \\leq X \\leq 5,\n",
    "\\end{align*}\n",
    "and can be seen in the plot below.\n",
    "\n",
    "<img src='Week1_plots/pdf_cdf_example.png' align='center' width = 1200>\n",
    "\n",
    "*PDF and CDF for the previously described distribution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CDF is analytically invertible; we simply need to rearrange $F(X) = u$ to find a function in terms of $u$ to generate samples. We can find that the inverse fun ction is therefore given by\n",
    "\n",
    "\\begin{equation*}\n",
    "F^{-1}(u) = \\sqrt[3]{2\\cdot5^3\\left(u - \\frac{1}{2}\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the cube root always has one real root (the other two are complex) and this is the one we must take for our inverse. For $u\\ge 0.5$ the root is real and positive while for $u\\le 0.5$ the real root is negative (think of the solutions to $(+1)^{1/3}$ and $(-1)^{1/3}$). \n",
    "\n",
    "We define a method to give the inverse CDF function $F^{-1}(u) $ we just found above. We then use `numpy.random` to generate random numbers from the uniform distribution. Finally we our inverse function to our random numbers to produce our artificial data which should now look like the original pdf $f(X) = (3/250) X^2$ plotted earlier. \n",
    "\n",
    "<!-- The code cell below generates 100,000 draws from the uniform distribution $U(0,1)$ and applies the inverse function to generate the desired distribution. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw random numbers from pdf:- f(X)= (3/250) X^2 for -5<X<5 otherwise zero\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "# This is the number of measurements in the artifical data\n",
    "number_observations = 10000\n",
    "\n",
    "def inv(u):\n",
    "    return np.cbrt(2*5**3*(u - 0.5))\n",
    "\n",
    "# For reproducibility, I like to fix the seed for the random number generator, any number is as good as any other\n",
    "rng_seed = 0\n",
    "rng = np.random.default_rng(rng_seed) # This is a random number generator (hence \"rng\") method\n",
    "samples = rng.uniform(0,1,size = number_observations) # These are the u values\n",
    "\n",
    "draws = inv(samples) # These are the X=f^{-1}(u) values for each u value \n",
    "bins = np.linspace(-5,5,101) # split the space into equal size bins\n",
    "\n",
    "# Now find exact theoretical results for comparison.  \n",
    "def cdf(X):\n",
    "    return X*X*X/250 + 0.5\n",
    "\n",
    "bin_centre=[(bins[b]+bins[b-1])/2 for b in range(1,len(bins))] # use midpoint to show expected result for each bin\n",
    "exact_bin_value=[number_observations *(cdf(bins[b])-cdf(bins[b-1]) ) for b in range(1,len(bins))] # Use cdf to find expected number in each bin\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (6,4),dpi = 150)\n",
    "vals, bins, patches = ax.hist(draws,bins = bins,fc='#D55E00',edgecolor='black')\n",
    "\n",
    "# This is the exct prediction for each bin from the pdf, comment it out if you want the raw results alone.\n",
    "ax.plot(bin_centre,exact_bin_value, \"+\", color='k')\n",
    "\n",
    "ax.set_xlabel('X',fontsize = 16)\n",
    "ax.set_ylabel('Frequency',fontsize = 16)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.4))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(100))\n",
    "ax.set_title('Inverse transform generated data',fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a fairly reasonable approximation of the PDF $f(X) = \\frac{3}{2\\cdot5^3}X^2$ (the black crosses show the predicted value for each bin).\n",
    "\n",
    "This method can only be used for functions that are analytically integrable and invertible. When we cannot do these analytically, we have to do it numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a PDF that is not normalised, you must also calculate the limits on $u$. For example, for $f(X) = X^2$ with $-5 \\leq X \\leq 5$, we define \n",
    "\n",
    "\\begin{equation*}\n",
    "u = F(X) = \\frac{x^3}{3} + \\frac{5^3}{3}\n",
    "\\end{equation*}\n",
    "\n",
    "We can then substitute in the limits of $X$ to find the limits of $u$; substituting in $X = -5$ and $X = 5$ gives us limits of $u = 0$ and $u = \\frac{2\\cdot5^3}{3}$. We can then generate values $u$ from the distribution $U(0,\\frac{2\\cdot5^3}{3})$ to sample our desired distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C2D5BD\"> \n",
    "\n",
    "OPTIONAL Exercise: add the theoretical prediction for the number expected in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inverse function\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate randomly sampled data\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply inverse function to randomly sampled data\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of generated samples\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Inverse Transform\n",
    "\n",
    "\n",
    "The function that you have may be difficult to either integrate or invert. Some functions, like the Gaussian distribution, are only integrable numerically, or you may have data sampled only at certain points. We can instead:\n",
    "\n",
    "* Generate samples from the known PDF (if PDF is analytic), or use our sampled data\n",
    "<br>\n",
    "\n",
    "* Find the cumulative sum of these samples & normalise it to approximate the CDF\n",
    "<br>\n",
    "\n",
    "* Invert the numerical CDF $F(X_i)$ by interpolating with $F(X_i)$ as the \"X\" value and $X_i$ as the \"Y\" value. This gives the inverse function\n",
    "<br>\n",
    "\n",
    "* Generate samples from the uniform distribution U(0,1) and apply the interpolated inverse function to them to generate samples\n",
    "\n",
    "We will use an example from nuclear physics to demonstrate this. \n",
    "\n",
    "* The energy spectrum of fission neutron sources is an experimentally measured quantity, with standards published in reference documents \n",
    "<br>\n",
    "\n",
    "* There are distributions designed to fit these spectra (e.g. the Watt distribution) but they have been determined empirically rather than from theory\n",
    "\n",
    "You have been provided with a file called \"cf_spectra.npy\", which contains measured neutron rates from a $^{252}\\text{Cf}$ source in different energy bins as published in ISO standards (internationally agreed standard values). These rates are measured from a given source so are currently not normalised, and are dependent on the size of the energy bin. \n",
    "\n",
    "The energy bins range across 10 orders of magnitude, from $10^{-8}$ to $10^2$ MeV. To visualise this properly, we will need to plot on a log scale. A plot of the PDF can be seen below.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "By inverting the function analytically, generate a distribution with the PDF $f(X) = e^X$ for $-2 \\leq X \\leq 2$. Remember the following steps:\n",
    "\n",
    "* Calculate the inverse cumulative function\n",
    "* Generate randomly sampled data $u$\n",
    "* Apply inverse function to samples $u$\n",
    "* Histogram generated samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Week1_plots/cf_spectrum.png' align='center' width=600>\n",
    "\n",
    "*$^{252}\\textit{Cf}$ neutron energy spectrum. This is an unnormalised PDF.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file \"cf_spectra.npy\" contains X values, which are the centers of the energy bins used for measurement, and y values, which are the rate measured in each energy bin. Note that `*.npy` files are usually binary files used to store numpy arrays.  This data defines a distribution function approximately as we are not given the value for every possible real energy value. The aim of the code below is to use this incomplete data on the real distribution to generate an artifical data set of typical energy measurements by sampling from this reference distribution given in the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key step as always is inverting the cdf, finding $F^{-1}$. To do this we will use the trick of looking at the data with the axes switched, so energy as the vertical axis and the cdf $F$ used as the horizontal axis. \n",
    "\n",
    "To see this consider any $x$ and $y$ values linked by the function $F$ so that $y=F(x)$. In a traditional plot we would put $y$ as the vertical axis of our plot and $x$ as the horizontal axis. Mathematically, the definition of the inverse function $F^{-1}$ is that $F^{-1}$ maps the $y$ values back to the $x$ value, it is the inverse operation, so that by definition \n",
    "$$F^{-1}(y) = x \\, .$$\n",
    "So we can see that any plot of $x$ and $y$ values can be seen in the traditional way, the curve tells us the function $y=F(x)$ as a function of the horizontal axis $x$ values. Alternatively, we can view the same plot think of the vertical axis $y$ values as the input and then the curve gives us the $x=F^{-1}$ values on the horizontal axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "NOTE:- this trick for finding the inverse function *only* works for functions where for every input $x$ value there is a unique output value $y=F(x)$ *and* vice versa. This is always true for the CDF but rarely for other functions. For instance in the rate $r=f(E)$ vs energy $E$ plot above, at rate value of $r=0.3$ could come from two possible energy $E$ values. This means the $E=f^{-1}(0.3)$ is not a single well defined number so here this unnormalised PDF $f$ does not have a well defined inverse $f^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a value of energy from the input of any given cumulative probability $F(X)$ we will use a type of interpolation called a \"cubic spline\" to fill in the gaps in the data. We will see some details of this later in this notebook.  \n",
    "\n",
    "In the plot below we will see our data in this case describes a very smooth function so the interpolation works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "# Load the data; need to unpickle the dictionary\n",
    "cf_iso = np.load('cf_spectra.npy',allow_pickle = True).item() \n",
    "\n",
    "X = cf_iso['X']\n",
    "y = cf_iso['y']\n",
    "\n",
    "\"\"\" Define and normalise the cumulative distribution\n",
    " THIS CODE ONLY WORKS if the input data is sorted in order of X values\n",
    " with smallest X values first. It is here.\n",
    " If not, sort the data frame  cf_iso by the 'X' column using\n",
    " cf_iso.sort_values(by=['X'])\n",
    " Also note np.cumsum only works if the all except the first value \n",
    " are positive within numerical rounding errors\n",
    "\"\"\" \n",
    "ycdf = y.cumsum()    # This is not yet normalised but last value is the total\n",
    "ycdf = ycdf/ycdf[-1] # Use the last value to normalise values \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Interpolate the inverted function.\n",
    "\n",
    "Note the ordering of arguments in th next line of code.\n",
    "A smoothed version of the normalised would be ordered CubicSpline(X, ycdf).\n",
    "To get the smoothed inverse function that we want we switch the order of the arguments CubicSpline(ycdf, X).\n",
    "\"\"\"\n",
    "inv = CubicSpline(ycdf, X)\n",
    "\n",
    "# Check the quality of the spline\n",
    "\n",
    "fig, (ax0,ax1) = plt.subplots(2,1,figsize = (6,8),dpi = 150)\n",
    "ax0.plot(ycdf,X,'x', label='Original data',color='black')\n",
    "ax0.plot(ycdf,inv(ycdf),label='Interpolated data',ls='--',color='#D55E00')\n",
    "ax0.legend(loc='lower right')\n",
    "ax0.set_xlabel('$u$',fontsize = 16)\n",
    "ax0.set_ylabel('$F^{-1}(u)$',fontsize =16)\n",
    "ax0.set_yscale('log')\n",
    "ax0.set_title('Inverse cumulative distribution',fontsize = 20)\n",
    "ax0.xaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax0.yaxis.set_minor_locator(LogLocator(base = 10.0,numticks =14, subs = [0.2,0.4,0.6,0.8]))\n",
    "\n",
    "# Generate uniform samples and apply the inverse function to them to produce the distribution\n",
    "\n",
    "u = np.random.default_rng().uniform(0,1,size = 100000)\n",
    "\n",
    "samples = inv(u)\n",
    "\n",
    "# Plot histogram of generated distribution; manually specify bins using np.logspace\n",
    "\n",
    "bins = np.logspace(-8,2,100)\n",
    "\n",
    "ax1.hist(samples, bins = bins,color='#D55E00', label='Artificial data',)\n",
    "ax1.set_xlabel('Energy [MeV]',fontsize = 16)\n",
    "ax1.set_ylabel('Frequency',fontsize = 16)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_title('Generated distribution',fontsize = 20)\n",
    "ax1.set_xlim(1e-8,56)\n",
    "ax1.xaxis.set_major_locator(LogLocator(base = 10.0,numticks = 14))\n",
    "ax1.xaxis.set_minor_locator(LogLocator(base = 10.0,numticks = 14, subs = [0.2,0.4,0.6,0.8]))\n",
    "ax1.yaxis.set_minor_locator(MultipleLocator(400))\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "### An aside on interpolation\n",
    "\n",
    "When we work with the numerical inverse transform, we need to **interpolate** to find the inverse cumulative distribution. This is a way to fill gaps within the range of the data, e.g. if we only have some samples of a continuous function. \n",
    "    \n",
    "A related but distinct task is **extrapolation** where you use data to predict new results outside the range of the data e.g. to predict global temperatures in the future.\n",
    "\n",
    "The most common type of interpolation is **linear interpolation**, where we draw a straight line between each of our data points. While this can work in some cases, if our function is complex or our data samples aren't sufficiently dense (so the true function varies a lot between out points) we can make large errors by just using linear interpolation. In these situations, we may want to use a method called a **spline**.\n",
    "\n",
    "The basic idea of a spline is that we subdivide the space between each of our data points into multiple regions, where in each region we model the function with a polynomial. The choice of polynomial parameters and the specific splits are optimised to make the function as smooth as possible. The order of the spline is the order of the polynomial we use in each of these regions. Typically, a **cubic spline** is good enough for most purposes, but can over/understimate near extrema of a function. The plot below shows an example of linear interpolation and a cubic spline for the function $f(X) = \\cos(-X^2/9)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, non-examinable material on interpolation\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# x = np.array([0,0.5,1.0,1.5,2.0,2.5,3.0,3.7,4.2,4.9,5.1,5.4,5.8,6.0,6.2])\n",
    "x = np.linspace(-2,10,13)\n",
    "y = np.cos(-x**2/9)\n",
    "\n",
    "xs = np.linspace(-2,10,1000)\n",
    "\n",
    "y_linear = np.interp(xs, x, y)\n",
    "y_spline = CubicSpline(x, y)(xs) # CubicSpline returns a Spline object that we have to call with the points we want to evaluate at\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (10,4),dpi = 150)\n",
    "ax.scatter(x,y,color='black')\n",
    "ax.plot(xs, np.cos(-xs**2/9),color='black',label='True function')\n",
    "ax.plot(xs, y_linear, color='#D55E00',label='Linear interpolation')\n",
    "ax.plot(xs, y_spline, color='#56B4E9',label='Cubic spline')\n",
    "ax.set_xlabel('$X$',fontsize = 16)\n",
    "ax.set_ylabel('$y$',fontsize = 16)\n",
    "ax.set_title('Types of interpolation (non-examinable)',fontsize = 20)\n",
    "ax.legend(loc='lower left',fontsize = 12)\n",
    "ax.tick_params(direction='in',top=True,right=True,which='both',labelsize=12)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.4))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "# ax.set_xlim(-2,10.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\"> End of aside on interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "(a) Using the numerical inversion method, generate samples from the standard normal distribution (a Gaussian with $\\mu$ = 0 and $\\sigma$ = 1).\n",
    "\n",
    "Try plotting three different sets of generations, with different number of points generated, to see how the distribution builds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a standard normal distribution and calculate some numerical data\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative distribution from y data; remember to normalise\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate to find the inverse function; remember first argument should be cumulative distribution, then X\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from uniform distribution in range [0,1]\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply inverse function to samples and plot histogram\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Exercise 5(b). Here we have used a normalised standard normal distribution; do we need to normalise a distribution to use for generating Monte Carlo data? Put your answer in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2D5BD\">\n",
    "\n",
    "Exercise 5(c) OPTIONAL. It is hard to see how good these artificial sample are.  So let us use the cumulative distribution function. It takes a little more effort to do this. I use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_empirical(samples,X):\n",
    "    \"\"\" \n",
    "    This gives the empirical cumulative distribution of the data in \"samples\" for any given X value \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    samples = the data values as a numpy array\n",
    "    X = Value of the cdf we wish to know (can not also be a numpy array)   \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    The value of the cdf at X for the given data in samples\n",
    "    \"\"\"\n",
    "    return np.count_nonzero(samples<X)/len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now you could use the same samples from the uniform distribution\n",
    "created in exercise 2a above to generate artifical data.\n",
    "Then use cdf_empirical to find the CDF of the artifical data\n",
    "and compare by sight in a plot with the interpolated ycdf function.\n",
    "\"\"\" \n",
    "\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Accept-Reject Method [^](#index) <a id='accept-reject'></a>\n",
    "\n",
    "If the PDF $f(x)$ is too complicated even to invert numerically, then we can always use the accept/reject method. This is particularly useful for functions with discontinuities.  This Accept-Reject method is performed by according to the following steps:\n",
    "\n",
    "1. Generate a random number $X$ from a uniform distribution over the range of the desired PDF\n",
    "1. Generate a second random number $y$ from the uniform distribution $h(y)$ = $U(0,a)$ where $a$ is the maximum value of the function $f(X)$ within the range of $X$ values required.  \n",
    "1. If $y \\leq f(X)$, then accept this point; otherwise it is rejected\n",
    "1. Repeat from (1) until you have the number of points you want.\n",
    "1. The set of kept values are a set random values $\\{X\\}$ drawn from a probability distribution with pdf f(X) \n",
    "\n",
    "This method only accepts points that lie beneath the desired PDF; because regions of the PDF with higher probability have more \"vertical\" space available, the density of points accepted in these regions is increased compared to regions of low probability. \n",
    "\n",
    "For example, consider a density function defined as $f(X) = -X^2 + 4$ for the range $-2 \\leq X \\leq 2$. The code cell below shows how we can use the accept-reject method on this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function. Note this is NOT normalised.\n",
    "# Note for a more complicated function with discontinuities we just need more code with \"if\" statements\n",
    "def f(x):\n",
    "    return -x**2 + 4\n",
    "\n",
    "# We want to generate values within the range X=-2..2. \n",
    "# We need $a$ the maximum value of $f(X)$ in that range which we set by hand\n",
    "a_value = f(0.0)\n",
    "\n",
    "# For reproducibility, I like to fix the seed for the random number generator, any number is as good as any other\n",
    "rng = np.random.default_rng(0) \n",
    "\n",
    "# Generate uniform X samples in the region of interest\n",
    "x_samples = rng.uniform(-2,2,size = 5000)\n",
    "\n",
    "# Generate uniform y samples in U(0, max(f(X)) (where the max(f(X) here is at f(0))\n",
    "y_samples = rng.uniform(0,a_value,size = 5000)\n",
    "\n",
    "# Find which samples fulfill the criteria y <= f(X);\n",
    "# This is an array of True and False values \n",
    "accept = (y_samples <= f(x_samples))\n",
    "\n",
    "# Plot the PDF, envelope function, and some samples\n",
    "fig, ax = plt.subplots(1,1,figsize = (9,7))\n",
    "# Next draw box containing all the points which are drawn\n",
    "ax.plot([-2,-2,2,2],[0,a_value,a_value,0],ls='--',color='grey',label='Envelope function $h(y)$')\n",
    "# Next draw function\n",
    "ax.plot(np.linspace(-2,2,100),f(np.linspace(-2,2,100)),label='$f(X)$',color='black')\n",
    "ax.set_xlim(-2.2,2.2)\n",
    "ax.set_ylim(0,1.35*a_value) # leave a little space at the top for the legend\n",
    "ax.set_xlabel('$X$',fontsize = 18)\n",
    "ax.set_ylabel('$y$',fontsize = 18)\n",
    "ax.tick_params(labelsize = 14,which='both')\n",
    "# Next plot ALL the points tried as failed points \n",
    "ax.scatter(x_samples, y_samples, s = 1, label='Rejected samples', color='#56B4E9')\n",
    "# but then for the points we accept we plot another point to indicate accepted points as this will be on top and seen by us\n",
    "ax.scatter(x_samples[accept],y_samples[accept],s = 1,c='#D55E00',label = 'Accepted samples')\n",
    "ax.legend(loc='upper center',fontsize = 12)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that only the samples beneath the desired PDF $f(X)$ are accepted, while all other samples are rejected. We can then plot a histogram of these values to verify we have successfully generated data according to our PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (6,4),dpi = 150)\n",
    "ax.hist(x_samples[accept], bins = 20,color='#D55E00')\n",
    "ax.set_xlabel('$X$',fontsize = 18)\n",
    "ax.set_ylabel('Frequency',fontsize = 18)\n",
    "ax.tick_params(labelsize = 14,which='both')\n",
    "ax.set_title('Generated distribution',fontsize = 22)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, this approach is *not efficient*, as you generate far more data than is necessarily useful, depending on the shape of the function. In this case, we can find the fraction of the data that is accepted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_kept = accept.sum() # can sum boolean values to get number of True\n",
    "number_generated = len(x_samples)\n",
    "\n",
    "fraction_accepted = number_kept/number_generated\n",
    "print(fraction_accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have accepted about 67.0% of generated data points. This means we need to generate about 30% more data than we want, for this function; so for 100,000 data points generated according to the distribution, we must generate 130,000 data points in total. This value will depend greatly on the specifics of the PDF in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "Using the accept-reject method, generate a distribution with 100,000 entries for $f(X) = e^{-X^2/2}$ for $-3 \\leq X \\leq 3$. Remember the following steps:\n",
    "\n",
    "* Generate random uniform samples in the range for desired PDF\n",
    "* Generate random uniform samples from distribution $h(y) = U(0, a)$ where a is the maximum value of the desired PDF in the interval\n",
    "* Find samples for which $y \\leq f(X)$\n",
    "* Plot histogram of the accepted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uniform samples in PDF range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uniform samples from h(y)\n",
    "\n",
    "# OPTIONAL try y samples from a larger range, e.g. a_value = max(f(x)) * 1.1 \n",
    "# OPTIONAL try y samples from a smaller range, e.g. a_value = max(f(x)) * 0.9 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find accepted samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "\n",
    "# OPTIONAL instead of plotting all the values first \n",
    "#          can you plot the rejected values only, \n",
    "#          e.g. try the \"~\" NOT function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"> End of exercise 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, python has a lot of functions for generating data from some default distributions for you. Common choices include `numpy.random`, which has generators for many common distributions, or `scipy.stats`, which implements probability distributions that you can visualise (as we have done earlier) with many common methods between different distributions for producing e.g. the PDF, generating random samples, calculating the first four moments of the distribution (mean, variance, skew and kurtosis), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have discussed Monte Carlo methods for data generation, including:\n",
    "\n",
    "* The inverse transform method\n",
    "* The numerical inverse transform method\n",
    "* The accept-reject method\n",
    "\n",
    "The following section covers the exercises you should work through this week.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises [^](#outline)<a id='section-7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Generate a distribution of 1 million entries with a pdf of $\\sin(x)$ between 0 and $\\pi$. Do this by:\n",
    "\n",
    "1. Inverting the analytic integral\n",
    "1. Integrating numerically\n",
    "1. Using the accept-reject method\n",
    "\n",
    "Jupyter Notebooks let you time how long a cell runs by putting `%%timeit` at the start of a cell. Use this to time each method you use. Which one is fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytic inverse\n",
    "\n",
    "%%timeit\n",
    "\n",
    "# Calculate and define the inverted analytic integral\n",
    "\n",
    "def inv(x):\n",
    "    return  None #your code here\n",
    "\n",
    "\n",
    "# Generate uniform samples\n",
    "\n",
    "u =  None\n",
    "\n",
    "# Apply inverted function to uniform samples\n",
    "\n",
    "samples =  None\n",
    "\n",
    "# Plot histogram of samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical inverse\n",
    "\n",
    "%%timeit \n",
    "\n",
    "# Generate X values IN ASCENDING ORDER and corresponding y values to use in the region of interest\n",
    "number_interpolation_points = 100\n",
    "\"\"\" The next line should work in general \n",
    "EXCEPT here the last x_values =\\pi and so the last y_value is y=sin(\\pi)=0.\n",
    "As we are inverting the plot of F^{-1}(ycdf) vs y is behaving like an infinite gradient\n",
    "and CubicSpline does not like it. \n",
    "SOLUTION. Leave the last point off using the endpoint=False\n",
    "\"\"\"\n",
    "X_interpolation = np.linspace(0, np.pi, num=number_interpolation_points, endpoint=False )\n",
    "\n",
    "# Calculate f(X) for each point\n",
    "\n",
    "y_interpolation = None\n",
    "\n",
    "# Calculate and normalise cumulative distribution\n",
    "\n",
    "ycdf = None\n",
    "\n",
    "# Interpolate to find our numerical inverse function\n",
    "\n",
    "inv =  None\n",
    "\n",
    "# Now use our numerical inverse function to generate artifical data\n",
    "# Generate uniform samples\n",
    "\n",
    "u =  None\n",
    "\n",
    "\n",
    "# Apply interpolated inverse function to uniform samples\n",
    "\n",
    "samples = None\n",
    "\n",
    "\n",
    "# Plot histogram of samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept-reject method\n",
    "\n",
    "%%timeit\n",
    "\n",
    "# Generate uniform X samples in the region of interest\n",
    "\n",
    "x_samples =  None\n",
    "\n",
    "# Generate uniform y samples in U(0, max(f(X)) (where the max(f(X) here is at f(0))\n",
    "\n",
    "y_samples =  None\n",
    "\n",
    "# Find which samples fulfill the critera y <= f(X); the condition used here is requiring that f(X) is not < y\n",
    "\n",
    "keep =  None\n",
    "\n",
    "# Find accepted samples\n",
    "\n",
    "accept =  None\n",
    "\n",
    "\n",
    "# Plot histogram of accepted samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 8\n",
    "\n",
    "One useful tool to illustrate the Central Limit Thoerem is a construct called a Galton machine. Such a machine is constructed as follows:\n",
    "\n",
    "* A board with offset rows of pegs, with collection bins at the bottom\n",
    "* A marble is dropped into the top of the board at the center, and left to fall to the bottom\n",
    "* At each peg the marble bounces either left or right by 1 position\n",
    "* When the marble reaches the bottom of the board it lands in one of the bins and the position is recorded\n",
    "\n",
    "The figure below illustrates such a machine.\n",
    "\n",
    "<img src='Week1_plots/galton_image.png' align='center' width=600>\n",
    "\n",
    "We can can simulate this action. At each peg, the ball as an equal probability of going left or right. The code cell below contains a function to simulate a single step in a Galton machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility, fix the seed for the random number generator, any number is as good as any other\n",
    "rng = np.random.default_rng(0) \n",
    "\n",
    "def galton_step(x):\n",
    "    r = rng.uniform(0,1)\n",
    "    if r<0.5:\n",
    "        return x-1 # Move left\n",
    "    else:\n",
    "        return x+1 # Move right\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Using this function and the code cells below, experiment with changing the number of layers and the number of trials of the Galton machine. Plot a histogram of the final position of the balls dropped through the Galton machine. \n",
    "\n",
    "You should find the result is distributed according to the standard normal distribution $N(\\mu,\\sigma)$ where $\\mu=0$ and $\\sigma = \\sqrt{t}$ where $t$ is the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 10\n",
    "n_trials = 10\n",
    "\n",
    "finish = []\n",
    "for n in range(n_trials):\n",
    "    x_position = 0 # start at the centre\n",
    "    for j in range(n_layers):\n",
    "        x_position = galton_step(x_position)\n",
    "    finish.append(x_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plotting code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C2D5BD\"> \n",
    "    \n",
    "#### Optional Exercise 8b.\n",
    "\n",
    "How is the Galton machine related to a [random walk](https://en.wikipedia.org/wiki/Random_walk) (drunkard's walk)?\n",
    "\n",
    "What value in the programme means we expect the mean to be zero, $\\mu=0$?\n",
    "\n",
    "Hence why do we expect the standard deviation to be $\\sigma=\\sqrt{L}$ where $L$ is the number of layers the ball has passed through?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C2D5BD\"> \n",
    "    \n",
    "*Optional answer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 9\n",
    "\n",
    "You may have encountered the famous [Monty Hall problem](https://en.wikipedia.org/wiki/Monty_Hall_problem) as an illustration of the application of [Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). The problem goes like:\n",
    "\n",
    "* On a gameshow, there are three doors and there is a prize behind one of these doors.\n",
    "* You first choose one door but you don't open it yet.\n",
    "* The host opens one of the two remaining doors to show you there was nothing behind that door.\n",
    "* You then have the option to stick with your original choice, or to switch to the remaining door.\n",
    "* If you open the door with the prize behind it, you win the prize.\n",
    "\n",
    "After some though you can show that the probability of winning the prize is 1/3 if you stick with your original choice of door, while it is 2/3 if you swap doors. This may seem a bit counter-intuitive and this is one reason why it is used to illustrate why a Bayesian approach to probability can be extrememly powerful though for such a simple example you can reasch the same conclusions in other ways. \n",
    "    \n",
    "Here, we will use this problem to illustrate how you might use a numerical simulation of the game to arrive at the same result by studying how the number of successes and losses converges as we make more measurements (repeat the game many times). \n",
    "\n",
    "The code cell below defines a function to \"play\" the Monty Hall game; you choose a strategy of either \"stick\" or \"swap\" with the initial (random) choice of door. The doors are labelled A, B and C. Much like with the Galton problem, we can can simulate this action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prize_boxes = ['A','B','C']\n",
    "\n",
    "def playmontyhall(strategy,ngames=1, seed=0):\n",
    "    \"\"\"\n",
    "    Plays the Monty Hall game. \n",
    "    If only one game played then describes result of game game on screen\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    strategy = \"stick\" or \"swap\" \n",
    "    ngames = Number  of games to play (default 1)\n",
    "    seed = random number seed (default 0)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    List of strings, either 'win' or ' lose' giving results of the game.\n",
    "    \n",
    "    \"\"\"\n",
    "    # For reproducibility, I like to fix the seed for the random number generator, any number is as good as any other\n",
    "    rng = np.random.default_rng(seed) \n",
    "\n",
    "    games = []\n",
    "    for n in range(ngames) :\n",
    "        prize = rng.choice(prize_boxes)\n",
    "\n",
    "        # player chooses a box\n",
    "        player_choice = rng.choice(prize_boxes)\n",
    "\n",
    "        # host opens a box from remaining boxes.\n",
    "        remaining_choices = [c for c in prize_boxes if c not in [prize,player_choice]]\n",
    "        host_choice = rng.choice(remaining_choices)\n",
    "        remaining_box = [c for c in prize_boxes if c not in [host_choice,player_choice]]\n",
    "\n",
    "        # now player can swap or stick\n",
    "        if strategy == \"swap\":\n",
    "            original_choice = player_choice\n",
    "            player_choice = remaining_box[0]\n",
    "            if ngames <=1 : print(\"Player chose box {} originally, host opened box {}, player swapped to box {}\".format(original_choice,host_choice,player_choice))\n",
    "        else:\n",
    "            if ngames <=1 : print(\"Player chose box {}, host opened box {}\".format(player_choice,host_choice))\n",
    "        if ngames <=1 :\n",
    "            print(\" .... prize was in box {}\".format(prize))\n",
    "            if player_choice==prize:\n",
    "                print(\" -----> WIN!\")\n",
    "            else: \n",
    "                print(\" -----> LOSE :(\")\n",
    "\n",
    "    if player_choice==prize: \n",
    "        games.append(\"win\")\n",
    "    else: \n",
    "        games.append(\"lose\")\n",
    "    return games\n",
    "\n",
    "playmontyhall('stick') # actually anything but \"swap\" gives a stick strategy\n",
    "playmontyhall('swap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "You can pass the ngames argument to run the simulation multiple times in one go. \n",
    "\n",
    "Using this code, running the simulation 10 times to find the number of successes and failures for each strategy. \n",
    "\n",
    "* What happens if you increase the number of times you run the simulation? \n",
    "\n",
    "If you divide the number of successes by the total number of tries you can find the probability of success for each strategy. As you increase the number of times you run the simulation, you should find this tends towards the Bayes theorem result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "ngames = None\n",
    "\n",
    "# Run simulation\n",
    "\n",
    "\n",
    "\n",
    "# Calculate probability of success for each strategy\n",
    "\n",
    "p_success_stick =  None\n",
    "\n",
    "\n",
    "p_success_swap =  None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C2D5BD\"> \n",
    "    \n",
    "#### Optional Exercise 9b.\n",
    "\n",
    "What do you expect the difference to be between the result you find for a finite number of games and the predicted strategy? Hint: what is the distribution for finding $k$ wins in $n$ games? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C2D5BD\"> \n",
    "    \n",
    "*Your optional answer to optional exercise 9b.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 10\n",
    "\n",
    "Using the concrete dataset from Section 2, perform the following actions:\n",
    "\n",
    "* Use `query` to find all rows with Age $\\lt 30$, and another `query` to separately find all rows with Age $\\geq 30$.\n",
    "* Plot a histogram of Strength for these two DataFrames on the same axes. Make sure to use the same bins for both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two DataFrames using query\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 11\n",
    "\n",
    "Using what you know about plot formatting, reproduce the two histograms you plotted in Exercise 10 but with better formatting. Remember to consider:\n",
    "\n",
    "* Clear plot colours\n",
    "* Well-formatted legend\n",
    "* Appropriately sized axis labels, tick labels, and title\n",
    "* A grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> \n",
    "<a name=\"exercises\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exercises [^](#outline)<a id='moreexercises'></a>\n",
    "\n",
    "These exercises are here for you to get more experience using Jupyter notebooks, `pandas` and `matplotlib`. In each section, code cells have been given to help prompt you on each step to work through, but feel free to make your own steps if you want - just make sure you work through each of the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 12\n",
    "\n",
    "Using the concrete dataset, pick two of the quantities in the DataFrame and make a scatter plot. Remember the following plotting points:\n",
    "\n",
    "* Appropriate axis labels and title\n",
    "* Title, axis label and tick label fontsizes\n",
    "* Figure size & resolution\n",
    "* Tick formatting\n",
    "\n",
    "Save your figure using `plt.savefig` with an appropriate file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concrete data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and save your plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 13\n",
    "\n",
    "Using the plot you have just saved, display your plot in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise 14\n",
    "\n",
    "Using the concrete data, make an array of histograms of the concrete descriptive features, using `plt.subplots`. The descriptive features are the following DataFrame columns:\n",
    "\n",
    "* Cement\n",
    "* Blast Furnace Slag\n",
    "* Fly Ash\n",
    "* Water\n",
    "* Superplasticizer\n",
    "* Coarse Aggregate\n",
    "* Fine Aggregate\n",
    "* Age\n",
    "\n",
    "Remember to format your plots carefully. `plt.subplots` returns a `Figure` object and an array of `Axes`, which you can index to plot on/format individually. Make sure to consider the following formatting points:\n",
    "\n",
    "* Size of axis and tick labels\n",
    "* Colours\n",
    "* Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define figure and subplots\n",
    "\n",
    "\n",
    "\n",
    "# Plot each histogram\n",
    "\n",
    "\n",
    "\n",
    "# Add labels to each axis\n",
    "\n",
    "\n",
    "\n",
    "# Edit tick parameters on each axis\n",
    "\n",
    "\n",
    "\n",
    "# Add a title\n",
    "\n",
    "\n",
    "\n",
    "# Save your plot\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys60022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
